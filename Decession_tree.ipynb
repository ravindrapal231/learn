{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGHDv9nVoA9t1ZeVEnnRw7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ravindrapal231/learn/blob/main/Decession_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: What is a Decision Tree, and how does it work in the context of classification?\n",
        "**Answer:-**\n",
        "A Decision Tree is a type of supervised machine learning algorithm used for classification and regression tasks. In the context of classification, a decision tree is used to predict the class label of input data by learning simple decision rules from the data features.\n",
        "What is a Decision Tree?\n",
        "Think of it like a flowchart:\n",
        "•\tEach internal node represents a decision on a feature (e.g., “Is age > 30?”).\n",
        "•\tEach branch represents the outcome of the decision (Yes or No).\n",
        "•\tEach leaf node represents a class label (e.g., “Approved” or “Denied”).\n",
        "How Does It Work (in Classification)?\n",
        "1.\tStart at the Root Node:\n",
        "o\tThe tree begins with the root node, which considers all training data.\n",
        "2.\tSelect the Best Feature to Split:\n",
        "o\tThe algorithm chooses the feature that best separates the data into different classes.\n",
        "o\tMetrics used for this include:\n",
        "\tGini Impurity\n",
        "\tEntropy / Information Gain\n",
        "\tChi-square, etc.\n",
        "3.\tSplit the Dataset:\n",
        "o\tBased on the best feature, the dataset is split into subsets.\n",
        "o\tThe process is recursively repeated on each subset.\n",
        "4.\tStopping Conditions:\n",
        "o\tAll data points in a node belong to the same class.\n",
        "o\tThe tree reaches a maximum depth.\n",
        "o\tA minimum number of samples per node is reached.\n",
        "5.\tPrediction:\n",
        "o\tFor a new data point, the decision tree follows the path from the root to a leaf by evaluating the conditions at each node.\n",
        "o\tOnce a leaf is reached, the class label of the leaf is returned as the prediction.\n",
        "Example:\n",
        "If Age > 30:\n",
        "    If Income > 50K: → Approve Loan\n",
        "    Else: → Deny Loan\n",
        "Else:\n",
        "    → Deny Loan\n",
        "Advantages:\n",
        "•\tEasy to understand and interpret.\n",
        "•\tHandles both numerical and categorical data.\n",
        "•\tRequires little data preprocessing.\n",
        "Disadvantages:\n",
        "•\tProne to overfitting if not properly pruned.\n",
        "•\tCan be unstable with small changes in data.\n",
        "•\tBiased towards features with more levels.\n",
        "Use Cases:\n",
        "•\tMedical diagnosis (e.g., classifying types of diseases)\n",
        "•\tCredit scoring\n",
        "•\tCustomer segmentation\n",
        "•\tSpam filtering\n"
      ],
      "metadata": {
        "id": "Oej6tnTonf49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "**Answer :-**\n",
        "Great question! Gini Impurity and Entropy are two common impurity measures used in Decision Trees to decide how to split nodes. These measures evaluate how “pure” or “impure” a node is — that is, how mixed the classes are within it. The goal is to reduce impurity as much as possible with each split.\n",
        "\n",
        "**Gini Impurity**\n",
        "Definition:\n",
        "Gini Impurity measures the probability of misclassifying a randomly chosen element from the set if it were labeled randomly according to the distribution of labels in the node.\n",
        "Formula:\n",
        "For a node with KKK classes:\n",
        "Gini=1−∑i=1Kpi2\\text{Gini} = 1 - \\sum_{i=1}^{K} p_i^2Gini=1−i=1∑Kpi2\n",
        "Where:\n",
        "•\tpip_ipi = proportion of class iii instances in the node\n",
        " Intuition:\n",
        "•\tGini = 0 → Node is pure (all samples belong to one class)\n",
        "•\tGini closer to 0.5 or 0.67 (for binary or multiclass) → Higher impurity\n",
        "________________________________________\n",
        " **Entropy (Information Gain)**\n",
        " Definition:\n",
        "Entropy comes from information theory. It measures the level of disorder or uncertainty in a node.\n",
        "Formula:\n",
        "Entropy=−∑i=1Kpilog⁡2(pi)\\text{Entropy} = - \\sum_{i=1}^{K} p_i \\log_2(p_i)Entropy=−i=1∑Kpilog2(pi)\n",
        "Intuition:\n",
        "•\tEntropy = 0 → Pure node\n",
        "•\tEntropy is maximum when classes are evenly distributed (e.g., 50/50 in binary classification → Entropy = 1)\n",
        "________________________________________\n",
        "How They Impact Splits\n",
        "The Role in Splitting:\n",
        "At each node, the algorithm:\n",
        "1.\tCalculates the impurity before the split.\n",
        "2.\tTries different features and thresholds to split the data.\n",
        "3.\tCalculates the weighted average impurity of child nodes after each split.\n",
        "4.\tChooses the split that maximizes impurity reduction.\n",
        "Impurity Reduction (for Entropy):\n",
        "Information Gain=Entropyparent−Weighted Entropychildren\\text{Information Gain} = \\text{Entropy}_{\\text{parent}} - \\text{Weighted Entropy}_{\\text{children}}Information Gain=Entropyparent−Weighted Entropychildren\n",
        "For Gini, a similar logic is applied (though not called \"information gain\").\n",
        "\n",
        "Gini vs. Entropy: Key Differences\n",
        "Feature\tGini Impurity\tEntropy (Information Gain)\n",
        "Computational Cost\tFaster (no log calculation)\tSlower due to logarithm\n",
        "Split Preference\tPrefers larger partitions\tMore sensitive to class balance\n",
        "Result Differences\tOften similar splits\tSlightly different in some cases\n",
        "In practice: Gini is often the default (e.g., in scikit-learn), and both give very similar trees.\n",
        "\n",
        " Example:\n",
        "Imagine a node with:\n",
        "•\t4 \"Yes\" samples\n",
        "•\t6 \"No\" samples\n",
        "Then:\n",
        "•\tpYes=0.4,pNo=0.6p_{Yes} = 0.4, p_{No} = 0.6pYes=0.4,pNo=0.6\n",
        "Gini:\n",
        "1−(0.42+0.62)=1−(0.16+0.36)=0.481 - (0.4^2 + 0.6^2) = 1 - (0.16 + 0.36) = 0.481−(0.42+0.62)=1−(0.16+0.36)=0.48\n",
        "Entropy:\n",
        "−(0.4log⁡20.4+0.6log⁡20.6)≈0.971- (0.4 \\log_2 0.4 + 0.6 \\log_2 0.6) \\approx 0.971−(0.4log20.4+0.6log20.6)≈0.971\n"
      ],
      "metadata": {
        "id": "lzVwsX8to7V8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3**: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each?\n",
        "\n",
        "**Answer**: -\n",
        "Pre-Pruning and Post-Pruning are two techniques used in decision tree learning to avoid overfitting and improve generalization.\n",
        "________________________________________\n",
        "Pre-Pruning (Early Stopping)\n",
        "Definition:\n",
        "In pre-pruning, the decision tree building process is stopped early, before it becomes too complex. The algorithm halts further splitting of a node if certain conditions are met.\n",
        "Common criteria:\n",
        "•\tMaximum tree depth is reached.\n",
        "•\tMinimum number of samples required to split a node is not met.\n",
        "•\tInformation gain from a split is below a threshold.\n",
        "Practical Advantage:\n",
        "Reduces training time by preventing unnecessary computations and keeps the model simple from the beginning.\n",
        "Example use case: In real-time systems (e.g., fraud detection) where fast training and low memory usage are important.\n",
        "________________________________________\n",
        "Post-Pruning (Pruning After Full Growth)\n",
        "Definition:\n",
        "In post-pruning, the decision tree is first allowed to grow fully, and then unnecessary branches are removed based on validation performance.\n",
        "Common techniques:\n",
        "•\tReduced Error Pruning\n",
        "•\tCost Complexity Pruning (used in CART)\n",
        "Practical Advantage:\n",
        "Improves generalization by removing parts of the tree that do not provide significant predictive power on validation data.\n",
        "Example use case: In applications where accuracy is more critical than training speed, like medical diagnosis, post-pruning leads to more reliable predictions.\n",
        "\n",
        " Summary Table\n",
        "Feature\tPre-Pruning\tPost-Pruning\n",
        "When applied\tDuring tree construction\tAfter tree is fully grown\n",
        "Goal\tAvoid growing complex trees\tSimplify complex trees\n",
        "Advantage\tFaster training, simpler models\tBetter generalization, improved accuracy\n",
        "Risk\tMay underfit the data\tHigher training time\n"
      ],
      "metadata": {
        "id": "jXQhdHR2qSib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4**: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "**Answer**: -\n",
        "Information Gain (IG) is a metric used to measure how well a feature splits the data in a decision tree. It quantifies the reduction in entropy (i.e., disorder or uncertainty) after a dataset is split based on a particular attribute.\n",
        "\n",
        "Entropy Recap\n",
        "Entropy measures the impurity or randomness in a dataset:\n",
        "Entropy(S)=−∑i=1npilog⁡2pi\\text{Entropy}(S) = -\\sum_{i=1}^{n} p_i \\log_2 p_iEntropy(S)=−i=1∑npilog2pi\n",
        "Where:\n",
        "•\tSSS is the dataset.\n",
        "•\tpip_ipi is the proportion of class iii in SSS.\n",
        "________________________________________\n",
        "Formula for Information Gain\n",
        "Information Gain(S,A)=Entropy(S)−∑v∈Values(A)∣Sv∣∣S∣⋅Entropy(Sv)\\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v)Information Gain(S,A)=Entropy(S)−v∈Values(A)∑∣S∣∣Sv∣⋅Entropy(Sv)\n",
        "Where:\n",
        "•\tSSS: original dataset\n",
        "•\tAAA: attribute used for the split\n",
        "•\tSvS_vSv: subset of SSS where attribute AAA has value vvv\n",
        "\n",
        "Why is Information Gain Important?\n",
        "•\tIt helps the decision tree algorithm choose the best attribute for splitting at each node.\n",
        "•\tThe attribute with the highest Information Gain is selected, as it maximizes the reduction in uncertainty.\n",
        "•\tThis leads to a tree that is more efficient and accurate in classifying data.\n",
        "\n",
        "Example\n",
        "Imagine a dataset of students with the target variable “Pass/Fail” and attributes like “Study Hours” and “Attendance.”\n",
        "If splitting the data on \"Study Hours\" reduces entropy significantly (i.e., makes the classes more pure), then \"Study Hours\" has high Information Gain and is a better choice for the root or a split node.\n",
        "________________________________________\n",
        "Summary\n",
        "Concept\tDescription\n",
        "Information Gain\tReduction in entropy after a dataset is split on an attribute\n",
        "Purpose\tTo find the most informative feature for the next split in the tree\n",
        "Benefit\tLeads to simpler, more accurate, and more generalizable decision trees\n"
      ],
      "metadata": {
        "id": "A_waILRSqkJV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5**: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "**Answer:**-\n",
        "Decision Trees are widely used in many practical domains due to their interpretability and versatility. Below are some common real-world applications, along with key advantages and limitations.\n",
        "Real-World Applications\n",
        "1.\tHealthcare\n",
        "o\tUse: Diagnosing diseases based on symptoms and test results.\n",
        "o\tExample: Predicting whether a patient has diabetes based on age, BMI, and glucose levels.\n",
        "2.\tFinance\n",
        "o\tUse: Credit scoring and loan approval.\n",
        "o\tExample: Classifying loan applicants as low or high risk based on income, employment status, and credit history.\n",
        "3.\tMarketing\n",
        "o\tUse: Customer segmentation and targeting.\n",
        "o\tExample: Predicting whether a customer will respond to a marketing campaign.\n",
        "4.\tE-commerce & Retail\n",
        "o\tUse: Product recommendation and demand forecasting.\n",
        "o\tExample: Predicting which products a user is likely to buy next.\n",
        "5.\tEducation\n",
        "o\tUse: Student performance prediction.\n",
        "o\tExample: Identifying students at risk of failing based on attendance and exam scores.\n",
        "6.\tManufacturing\n",
        "o\tUse: Fault detection and quality control.\n",
        "o\tExample: Predicting machine failure based on sensor readings.\n",
        "Main Advantages of Decision Trees\n",
        "Advantage\tDescription\n",
        "Easy to understand & interpret\tTree structure is visual and mirrors human decision-making.\n",
        "Requires little data preprocessing\tNo need to normalize features or scale data.\n",
        "Handles both numerical and categorical data\tCan work with a mix of feature types.\n",
        "Non-parametric model\tMakes no assumptions about data distribution.\n",
        "Can capture non-linear relationships\tEffective in complex decision boundaries.\n",
        "Limitations of Decision Trees\n",
        "Limitation\tDescription\n",
        "Prone to overfitting\tEspecially with deep trees and small datasets.\n",
        "Unstable\tSmall changes in data can lead to very different trees.\n",
        "Bias toward features with more levels\tFeatures with many unique values can dominate the splits.\n",
        "Less accurate compared to ensemble methods\tSingle trees are usually weaker than models like Random Forest or XGBoost.\n",
        "\n",
        "Summary\n",
        "Aspect\tDetails\n",
        "Applications\tHealthcare, finance, marketing, retail, education, manufacturing\n",
        "Advantages\tSimple, interpretable, handles various data types\n",
        "Limitations\tCan overfit, unstable, less accurate alone\n",
        "Let me know if you want a comparison between Decision Trees and ensemble methods like Random Forest or Gradient Boosted Trees.\n"
      ],
      "metadata": {
        "id": "LaCnvOXzqzzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Info:● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV). ● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV).#\n",
        "\n",
        "#Question 6: Write a Python program to:● Load the Iris Dataset● Train a Decision Tree Classifier using the Gini criterion● Print the model’s accuracy and feature importances#\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the Decision Tree Classifier with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Display feature importances\n",
        "importances = clf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPiJLGSVrB7N",
        "outputId": "06a30407-e0d6-4865-a15d-f68088b1492f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "          Feature  Importance\n",
            "petal length (cm)    0.906143\n",
            " petal width (cm)    0.077186\n",
            " sepal width (cm)    0.016670\n",
            "sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:● Load the Iris Dataset● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.#\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Decision Tree with max_depth = 3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# Fully-grown Decision Tree\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# Print the comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV-HKuRyrrgV",
        "outputId": "29bc9dd8-2c26-4b90-9511-819a10cb6bba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:● Load the Boston Housing Dataset ● Train a Decision Tree Regressor● Print the Mean Squared Error (MSE) and feature importances#\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Boston Housing dataset\n",
        "boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "feature_names = X.columns\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "# Display feature importances\n",
        "importances = regressor.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFzVxK9Ar_Sb",
        "outputId": "eab55e2d-af9b-4716-ecbb-e3f17c9e4b4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.42\n",
            "\n",
            "Feature Importances:\n",
            "Feature  Importance\n",
            "     RM    0.600326\n",
            "  LSTAT    0.193328\n",
            "    DIS    0.070688\n",
            "   CRIM    0.051296\n",
            "    NOX    0.027148\n",
            "    AGE    0.013617\n",
            "    TAX    0.012464\n",
            "PTRATIO    0.011012\n",
            "      B    0.009009\n",
            "  INDUS    0.005816\n",
            "     ZN    0.003353\n",
            "    RAD    0.001941\n",
            "   CHAS    0.000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:● Load the Iris Dataset● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV● Print the best parameters and the resulting model accuracy#\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Decision Tree model\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions and calculate accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(f\"Model Accuracy on Test Set: {accuracy:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqoontrjsadO",
        "outputId": "c5a72c30-fa7c-4b3e-ca46-f775736fc767"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy on Test Set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to:\n",
        "#● Handle the missing values #\n",
        "#● Encode the categorical features\n",
        "#● Train a Decision Tree model\n",
        "#● Tune its hyperparameters\n",
        "# Evaluate its performance And describe what business value this model could provide in the real-world setting.######\n",
        "\n",
        "\n",
        "**Answer:-**\n",
        "#Absolutely! Here's a step-by-step process for building a robust Decision Tree model for disease prediction in a healthcare setting with a large, mixed-type dataset containing missing values.\n",
        "#Step-by-Step Process\n",
        "#1. Handle Missing Values\n",
        "#•\tUnderstand the pattern: Use visualizations or summary statistics to explore if data is Missing Completely at Random (MCAR), Missing at Random (MAR), or Not at Random (MNAR).\n",
        "#•\tNumerical features:\n",
        "#o\tImpute with mean or median (use median for skewed data).\n",
        "#o\tAlternatively, use KNN imputer or IterativeImputer for more intelligent filling.\n",
        "#•\tCategorical features:\n",
        "#o\tImpute with mode.\n",
        "#o\tOr add a new category like \"Missing\" to represent absent information.\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Example for numerical\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "X_num = num_imputer.fit_transform(X[numerical_cols])\n",
        "\n",
        "# Example for categorical\n",
        "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "X_cat = cat_imputer.fit_transform(X[categorical_cols])\n",
        "\n",
        "\n",
        "\n",
        "#2. Encode Categorical Features\n",
        "#•\tLabel Encoding if the categories are ordinal (e.g., severity levels: mild, moderate, severe).\n",
        "#•\tOne-Hot Encoding for nominal categories (e.g., gender, city).\n",
        "#•\tUse ColumnTransformer to apply different encoders to different columns.\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "encoder = ColumnTransformer([\n",
        "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
        "], remainder='passthrough')\n",
        "\n",
        "\n",
        "#3. Train a Decision Tree Model\n",
        "#•\tSplit the data into training and testing sets.\n",
        "#•\tTrain a Decision Tree Classifier on the processed data.\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = DecisionTreeClassifier(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "#4. Tune Hyperparameters\n",
        "#Use GridSearchCV or RandomizedSearchCV to tune parameters like:\n",
        "#•\tmax_depth\n",
        "#•\tmin_samples_split\n",
        "#•\tmin_samples_leaf\n",
        "#•\tcriterion (gini or entropy)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 10, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "#5. Evaluate Model Performance\n",
        "#•\tUse metrics like:\n",
        "#o\tAccuracy – overall performance\n",
        "#o\tPrecision & Recall – especially important in healthcare (e.g., avoid false negatives!)\n",
        "#o\tF1-Score – balance of precision and recall\n",
        "#o\tConfusion Matrix – detailed error breakdown\n",
        "#o\tROC-AUC Score – for probabilistic performance\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Business Value of the Model\n",
        "#1.\tEarly Detection: Helps doctors and clinicians identify patients at high risk before symptoms worsen.\n",
        "#2.\tImproved Resource Allocation: Directs medical resources (e.g., lab tests, specialist referrals) to patients most in need.\n",
        "#3.\tPersonalized Treatment: Supports decisions for more tailored care plans based on patient risk profiles.\n",
        "#4.\tReduced Costs: Preventative care enabled by early predictions can reduce hospital admissions and treatment costs.\n",
        "#5.\tRegulatory and Compliance Reporting: Models help justify healthcare decisions and ensure data-driven transparency.\n",
        "#6.\tContinuous Learning: Feedback from model predictions can improve outcomes over time with retraining on new data.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "xob6CcpisuuH",
        "outputId": "adc7817d-3dee-4529-fbf6-96909c8eb54d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2919392076.py, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2919392076.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    **Answer:-**\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}