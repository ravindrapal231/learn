{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1:**  What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?\n",
        "\n",
        "**Answer:-**\n",
        "K-Nearest Neighbors (KNN):\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric and instance-based learning method, meaning it doesn‚Äôt assume an underlying probability distribution and makes predictions based on stored training data instead of building an explicit model.\n",
        "\n",
        "How KNN Works:\n",
        "Store training data: No explicit training phase (lazy learning).\n",
        "Calculate distance: For a new data point, calculate the distance (commonly Euclidean, Manhattan, or Minkowski) to all training points.\n",
        "Find nearest neighbors: Pick the k closest data points (neighbors).\n",
        "Make prediction:\n",
        "Classification: Assign the class most common among neighbors (majority voting).\n",
        "Regression: Take the average (or weighted average) of neighbors‚Äô values.\n",
        "\n",
        "KNN in Classification:\n",
        "\n",
        "Suppose we want to classify a new data point into one of the categories (e.g., \"cat\" or \"dog\").\n",
        "\n",
        "KNN finds the k nearest labeled data points.\n",
        "\n",
        "Uses majority voting ‚Üí whichever class occurs most among the k neighbors becomes the prediction.\n",
        "\n",
        "üëâ Example: If k=5 neighbors ‚Üí 3 cats + 2 dogs ‚Üí prediction = cat.\n",
        "KNN in Regression:\n",
        "Instead of voting, KNN takes the average (mean/median) of the neighbors‚Äô values.\n",
        "Can also use weighted KNN where closer neighbors have more influence (weights ‚àù 1/distance).\n",
        "\n",
        "üëâ Example: Predicting house prices:\n",
        "Neighbors‚Äô prices = [100k, 120k, 130k, 110k, 140k]\n",
        "Prediction = average = 120k\n",
        "Key Points about KNN:\n",
        "Choice of k:\n",
        "Small k ‚Üí sensitive to noise (overfitting).\n",
        "Large k ‚Üí smoother decision boundary but may underfit.\n",
        "Distance metric matters (Euclidean is most common).\n",
        "Feature scaling is important (since distance is sensitive to feature magnitude).\n",
        "Computation cost is high at prediction time (lazy learner).\n",
        "‚úÖ In summary:\n",
        "KNN Classification ‚Üí majority vote among k nearest neighbors.\n",
        "KNN Regression ‚Üí average (or weighted average) of neighbors‚Äô values.\n"
      ],
      "metadata": {
        "id": "0cDsQRCv7yCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2:** What is the Curse of Dimensionality and how does it affect KNN performance?\n",
        "\n",
        "**Answer:-**Curse of Dimensionality (CoD):\n",
        "\n",
        "The curse of dimensionality refers to the problems and challenges that arise when working with data in high-dimensional spaces (many features).\n",
        "As the number of dimensions (features) increases, data becomes sparse and distance measures become less meaningful.\n",
        "\n",
        "KNN relies heavily on distance metrics (like Euclidean distance) to find nearest neighbors.\n",
        "But in high dimensions:\n",
        "\n",
        "Distances lose meaning\n",
        "\n",
        "In high dimensions, the difference between the nearest and farthest neighbors becomes very small.\n",
        "That means \"close\" and \"far\" points become indistinguishable ‚Üí making KNN unreliable.\n",
        "\n",
        "üëâ Example: In a 1D line, points can be clearly close/far. But in 100D space, all points tend to look equally distant.\n",
        "Data sparsity\n",
        "As dimensions grow, the data spreads out more.\n",
        "To cover the space properly, you need exponentially more data.\n",
        "With limited data, KNN might not find truly \"similar\" neighbors.\n",
        "Increased computational cost\n",
        "More dimensions = more calculations per distance = slower predictions.\n",
        "Memory usage also grows since KNN stores the whole dataset.\n",
        "Feature scaling sensitivity\n",
        "If features are not normalized, dimensions with larger ranges dominate the distance calculation even more in high dimensions.\n",
        "Impact on KNN Performance:\n",
        "Classification: Decision boundaries become fuzzy ‚Üí higher misclassification rate.\n",
        "Regression: Predictions become less accurate since \"neighbors\" are not truly close.\n",
        "Overfitting risk: With high dimensions, small variations in data can mislead KNN.\n",
        "How to Mitigate Curse of Dimensionality in KNN:\n",
        "Dimensionality Reduction:\n",
        "PCA (Principal Component Analysis)\n",
        "t-SNE, UMAP for visualization\n",
        "Autoencoders (deep learning)\n",
        "Feature Selection: Keep only the most important features.\n",
        "Use distance metrics better suited for high dimensions (e.g., cosine similarity instead of Euclidean).\n",
        "In summary:\n",
        "The curse of dimensionality makes KNN struggle because in high dimensions, distance metrics lose their discriminatory power, data becomes sparse, and computation is heavy ‚Äî leading to poor accuracy and efficiency."
      ],
      "metadata": {
        "id": "owSdG95h8rZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3:** What is Principal Component Analysis (PCA)? How is it different from feature selection?\n",
        "\n",
        "**Answer:-**\n",
        "Principal Component Analysis (PCA):\n",
        "PCA is an unsupervised dimensionality reduction technique that transforms the data into a new coordinate system.\n",
        "It finds new axes (principal components) that capture the maximum variance in the data.\n",
        "These new axes are linear combinations of the original features.\n",
        "The first principal component captures the most variance, the second captures the next most (orthogonal to the first), and so on.\n",
        "üëâ So instead of working with all features, PCA projects data into fewer dimensions while preserving most of the information.\n",
        "Steps of PCA (simplified):\n",
        "Standardize the data (mean=0, variance=1).\n",
        "Compute the covariance matrix of features.\n",
        "Find eigenvalues & eigenvectors of the covariance matrix.\n",
        "Select top-k eigenvectors ‚Üí these form the new feature space.\n",
        "Transform original data into this lower-dimensional space.\n",
        "Example:\n",
        "Suppose we have 10 features.\n",
        "PCA might find that 90% of the variance is explained by just 3 new components.\n",
        "We can reduce from 10 ‚Üí 3 dimensions without losing much information.\n",
        "\n",
        "**PCA vs Feature Selection:**\n",
        "\n",
        "| **Aspect**           | **PCA (Feature Extraction)**                                                             | **Feature Selection**                                              |\n",
        "| -------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |\n",
        "| **Definition**       | Creates new features (principal components) as linear combinations of original features. | Selects a subset of existing features without changing them.       |\n",
        "| **Supervision**      | Unsupervised (does not use labels).                                                      | Can be supervised (based on importance to target) or unsupervised. |\n",
        "| **Interpretability** | Low ‚Üí principal components are abstract (combinations of many features).                 | High ‚Üí original features are kept, so they‚Äôre meaningful.          |\n",
        "| **Goal**             | Reduce dimensionality while keeping variance.                                            | Keep only the most relevant features.                              |\n",
        "| **Example**          | Turn 100 correlated features into 10 independent components.                             | From 100 features, keep only 15 most important ones.               |\n",
        "\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Feature selection = Choosing the best players from a team.\n",
        "PCA = Blending players‚Äô skills into a few \"super-players\" (combinations) that represent the whole team."
      ],
      "metadata": {
        "id": "86KGOcFh-ECC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 4:** What are eigenvalues and eigenvectors in PCA, and why are they important?\n",
        "\n",
        "**Answer:-**\n",
        "Eigenvalues and Eigenvectors in PCA\n",
        "\n",
        "When we apply PCA, the core step is computing the eigenvalues and eigenvectors of the covariance matrix of the data.\n",
        "\n",
        "Eigenvectors: Directions of the new feature space (principal components).\n",
        "\n",
        "Eigenvalues: Amount of variance (information) captured by each eigenvector.\n",
        "\n",
        "1. What is an Eigenvector?\n",
        "\n",
        "An eigenvector of a matrix is a vector whose direction does not change when the matrix is applied to it.\n",
        "\n",
        "In PCA, eigenvectors represent the axes along which the data varies the most.\n",
        "\n",
        "Each eigenvector = one principal component.\n",
        "\n",
        "üëâ Think of eigenvectors as directions of maximum spread in the data.\n",
        "\n",
        "2. What is an Eigenvalue?\n",
        "\n",
        "The eigenvalue tells us how much variance is captured along its corresponding eigenvector.\n",
        "\n",
        "Larger eigenvalue = more important principal component.\n",
        "\n",
        "Smaller eigenvalue = less variance (can be dropped in dimensionality reduction).\n",
        "\n",
        "üëâ Eigenvalues tell us how strong each direction (eigenvector) is.\n",
        "\n",
        "3. Why Are They Important in PCA?\n",
        "\n",
        "After finding eigenvectors and eigenvalues of the covariance matrix:\n",
        "\n",
        "Eigenvectors ‚Üí define the new feature axes (principal components).\n",
        "\n",
        "Eigenvalues ‚Üí tell us how much of the dataset‚Äôs variance each axis explains.\n",
        "\n",
        "We sort eigenvectors by their eigenvalues (from largest to smallest).\n",
        "\n",
        "Select the top-k eigenvectors ‚Üí project data onto them ‚Üí reduced dimensionality.\n",
        "\n",
        "Example (2D ‚Üí 1D PCA):\n",
        "\n",
        "Imagine data spread like an elongated cloud in 2D space:\n",
        "\n",
        "First eigenvector (with largest eigenvalue) aligns with the long axis of the cloud.\n",
        "\n",
        "Second eigenvector (with small eigenvalue) aligns with the narrow axis.\n",
        "\n",
        "üëâ If we only keep the first eigenvector, we reduce from 2D ‚Üí 1D while still keeping most variance.\n",
        "\n",
        "Analogy:\n",
        "\n",
        "Think of shining a flashlight on a 3D object to cast a 2D shadow:\n",
        "\n",
        "Eigenvectors = directions of projection (how we orient the flashlight).\n",
        "\n",
        "Eigenvalues = how much \"information\" or detail is captured in that projection.\n",
        "\n",
        "In summary:\n",
        "\n",
        "Eigenvectors = directions of maximum variance (new axes).\n",
        "\n",
        "Eigenvalues = magnitude of variance captured (importance of those axes).\n",
        "\n",
        "PCA keeps the eigenvectors with the largest eigenvalues ‚Üí reducing dimensions while retaining the most information."
      ],
      "metadata": {
        "id": "E6dh1stC_AT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5:** How do KNN and PCA complement each other when applied in a single pipeline?\n",
        "\n",
        "**Answer:-**\n",
        "How KNN and PCA Complement Each Other\n",
        "1. KNN‚Äôs Weakness\n",
        "\n",
        "KNN relies on distance metrics (Euclidean, Manhattan, etc.) to find neighbors.\n",
        "\n",
        "In high-dimensional space (curse of dimensionality):\n",
        "\n",
        "Distances lose meaning.\n",
        "\n",
        "Computation cost is high.\n",
        "\n",
        "Performance drops.\n",
        "\n",
        "2. PCA‚Äôs Strength\n",
        "\n",
        "PCA reduces dimensionality by projecting data into fewer principal components.\n",
        "\n",
        "It removes redundant/correlated features while preserving variance.\n",
        "\n",
        "It makes distances more meaningful because irrelevant/noisy dimensions are removed.\n",
        "\n",
        "3. Pipeline: PCA ‚Üí KNN\n",
        "\n",
        "A common workflow:\n",
        "\n",
        "Standardize the data (important because PCA and KNN are distance-based).\n",
        "\n",
        "Apply PCA ‚Üí reduce dimensionality, keep top-k components.\n",
        "\n",
        "Run KNN ‚Üí use distances in the reduced space for classification/regression.\n",
        "\n",
        "Why this works well:\n",
        "\n",
        "Noise reduction: PCA drops components with very low variance (often just noise).\n",
        "\n",
        "Efficiency: Fewer dimensions ‚Üí faster distance computations.\n",
        "\n",
        "Better generalization: KNN no longer struggles with meaningless high-dimensional distances.\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose you have 100 features for an image classification task:\n",
        "\n",
        "Many are correlated (pixel intensities).\n",
        "\n",
        "Running KNN directly = slow + inaccurate.\n",
        "\n",
        "Apply PCA ‚Üí reduce to 50 principal components (still capturing 95% variance).\n",
        "\n",
        "Run KNN in this new space ‚Üí faster, more accurate classification.\n",
        "\n",
        "Analogy:\n",
        "\n",
        "PCA = cleaning and compressing your data into the most informative essence.\n",
        "\n",
        "KNN = using that cleaned, compressed data to make neighbor-based decisions.\n",
        "\n",
        "‚úÖ In summary:\n",
        "\n",
        "PCA fixes KNN‚Äôs curse of dimensionality problem by reducing irrelevant/noisy features.\n",
        "\n",
        "Together:\n",
        "\n",
        "PCA = dimensionality reduction & noise filtering.\n",
        "\n",
        "KNN = classification/regression in the cleaned, compact feature space."
      ],
      "metadata": {
        "id": "53a0WZaMAh57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Dataset: Use the Wine Dataset from sklearn.datasets.load_wine(). Question 6: Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases. (Include your Python code and output in the code box below.) ##\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# KNN without scaling\n",
        "knn_no_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = knn_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# Apply scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# KNN with scaling\n",
        "knn_scaling = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_scaling = knn_scaling.predict(X_test_scaled)\n",
        "accuracy_scaling = accuracy_score(y_test, y_pred_scaling)\n",
        "\n",
        "print(\"Accuracy without scaling:\", accuracy_no_scaling)\n",
        "print(\"Accuracy with scaling:\", accuracy_scaling)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOLxseroBE3y",
        "outputId": "4a6364f0-a498-487d-ab79-8eae19e0cdcd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.7222222222222222\n",
            "Accuracy with scaling: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "Without feature scaling, KNN accuracy = 72.2%\n",
        "\n",
        "With feature scaling, KNN accuracy = 94.4%\n",
        "\n",
        "Since KNN is distance-based, scaling the features ensures all attributes contribute fairly to the distance calculation ‚Äî dramatically improving performance."
      ],
      "metadata": {
        "id": "R26zYQ1MCoLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Question 7: Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component. ##\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(\"Explained Variance Ratio of each Principal Component:\")\n",
        "print(explained_variance_ratio)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgbSfwvsBLtV",
        "outputId": "9146bca2-42f2-483f-90a0-84098e77af6b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained Variance Ratio of each Principal Component:\n",
            "[0.36198848 0.1920749  0.11123631 0.0706903  0.06563294 0.04935823\n",
            " 0.04238679 0.02680749 0.02222153 0.01930019 0.01736836 0.01298233\n",
            " 0.00795215]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "\n",
        "PC1 explains 36.2% of the variance.\n",
        "\n",
        "PC2 explains 19.2%.\n",
        "\n",
        "PC3 explains 11.1%.\n",
        "\n",
        "Together, the first 3 components explain ~66.5% of the dataset‚Äôs variance.\n",
        "\n",
        "So, instead of 13 original features, we can reduce to 3 PCs and still capture most of the information."
      ],
      "metadata": {
        "id": "mh653uc5CfdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Question 8: Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset. (Include your Python code and output in the code box below.) ##\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# KNN on original scaled dataset\n",
        "knn_original = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_original.fit(X_train, y_train)\n",
        "y_pred_original = knn_original.predict(X_test)\n",
        "accuracy_original = accuracy_score(y_test, y_pred_original)\n",
        "\n",
        "# PCA with top 2 components\n",
        "pca_2 = PCA(n_components=2)\n",
        "X_train_pca = pca_2.fit_transform(X_train)\n",
        "X_test_pca = pca_2.transform(X_test)\n",
        "\n",
        "# KNN on PCA-transformed dataset\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"Accuracy on original scaled dataset:\", accuracy_original)\n",
        "print(\"Accuracy on PCA (2 components):\", accuracy_pca)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbYsyhXxBYen",
        "outputId": "5c7f8e92-b1b2-431c-fc9f-5b3d963950bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on original scaled dataset: 0.9444444444444444\n",
            "Accuracy on PCA (2 components): 0.9629629629629629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:**\n",
        "\n",
        "Original scaled dataset (13 features): 94.4% accuracy\n",
        "\n",
        "PCA with 2 features (capturing ~55% variance): 96.3% accuracy üéâ\n",
        "\n",
        "üëâ Even though PCA reduced from 13 ‚Üí 2 dimensions, KNN performed slightly better, thanks to noise reduction and elimination of redundant features."
      ],
      "metadata": {
        "id": "GZCJ-YlZCZeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results. (Include your Python code and output in the code box below.) #\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# KNN with Euclidean distance (default, p=2)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# KNN with Manhattan distance (p=1)\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"Accuracy with Manhattan distance:\", accuracy_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RTPeBtxBmnS",
        "outputId": "aa303de1-86ab-4e81-cf4a-b87a39a24cc3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean distance: 0.9444444444444444\n",
            "Accuracy with Manhattan distance: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "Euclidean distance (94.4%) performed slightly better than\n",
        "\n",
        "Manhattan distance (90.7%) on the scaled Wine dataset.\n",
        "\n",
        "üëâ This makes sense because the Wine dataset has continuous-valued features where Euclidean distance is usually more effective than Manhattan."
      ],
      "metadata": {
        "id": "jiJVxSPhCMFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 10:** You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer. Due to the large number of features and a small number of samples, traditional models overfit. Explain how you would:\n",
        "‚óè Use PCA to reduce dimensionality\n",
        "‚óè Decide how many components to keep\n",
        "‚óè Use KNN for classification post-dimensionality reduction\n",
        "‚óè Evaluate the model\n",
        "‚óè Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data\n",
        "\n",
        "**Answer:-**\n",
        "1) Use PCA to reduce dimensionality\n",
        "\n",
        "Why PCA? Gene expression matrices (thousands of genes, < few hundred patients) are extremely high-dimensional with strong collinearity. PCA creates orthogonal components (linear combos of genes) that capture most variance while suppressing noise.\n",
        "\n",
        "Data hygiene: Always standardize features first (zero mean, unit variance). Without scaling, genes with larger ranges dominate distances and variance.\n",
        "\n",
        "2) Decide how many components to keep\n",
        "\n",
        "Use a two-step, leakage-safe strategy:\n",
        "\n",
        "Heuristic screen (unsupervised): Look at cumulative explained variance to identify a reasonable upper bound (e.g., keep components explaining 90‚Äì99% of variance).\n",
        "\n",
        "Supervised selection via CV: Within cross-validation, grid-search n_components (e.g., [10, 20, 50, 100, n_95]) and KNN hyperparameters together. This picks the smallest subspace that generalizes best (not merely the most variance).\n",
        "\n",
        "Key point for stakeholders: the number of components is chosen by cross-validated performance, not by eyeballing a plot, preventing overfitting.\n",
        "\n",
        "3) KNN for classification post-reduction\n",
        "\n",
        "In the reduced PCA space, KNN‚Äôs distance metric becomes more meaningful and robust.\n",
        "\n",
        "Tune:\n",
        "\n",
        "n_neighbors (e.g., 3‚Äì15),\n",
        "\n",
        "distance (Euclidean/Manhattan),\n",
        "\n",
        "weights (uniform vs distance).\n",
        "\n",
        "Keep everything in a single sklearn Pipeline so scaling/PCA are learned only from the training folds (no data leakage).\n",
        "\n",
        "4) Evaluate the model\n",
        "\n",
        "Stratified CV for model selection (preserves class ratios).\n",
        "\n",
        "Final held-out test set for unbiased performance.\n",
        "\n",
        "Report accuracy + macro F1 (macro F1 is crucial when classes are imbalanced).\n",
        "\n",
        "Include confusion matrix to show per-class behavior (important for clinical stakeholders).\n",
        "\n",
        "(Optional, if time allows) Permutation test or repeated CV to show stability; external validation if you have an independent cohort.\n",
        "\n",
        "5) Justify to stakeholders (robustness & clinical sense)\n",
        "\n",
        "Overfitting control: PCA reduces parameters and noise; CV chooses dimensions that actually generalize.\n",
        "\n",
        "Transparency: KNN is simple, and PCA loadings can be inspected to see which gene combinations drive components (you can attach gene sets / pathways).\n",
        "\n",
        "Reproducibility: One pipeline, with fixed random seeds and documented preprocessing.\n",
        "\n",
        "Operational practicality: Fast to train, fast to infer; easy to re-fit when new patients arrive."
      ],
      "metadata": {
        "id": "DkT1XDO8DMGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Code + example output (self-contained demo)\n",
        "\n",
        "##The code below simulates a gene-expression-like setting (thousands of features, few samples) to demonstrate the workflow. Replace the synthetic data block with your real matrix X (samples √ó genes) and labels y to run on your dataset. Printed results will vary slightly by split.##\n",
        "\n",
        "# ============================================\n",
        "# PCA ‚Üí KNN on high-dimensional gene expression\n",
        "# ============================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# --- 0) Get data ---\n",
        "# Replace this synthetic generator with your real X (n_samples x n_genes) and y (labels).\n",
        "# Example: X = your_matrix; y = your_labels\n",
        "X, y = make_classification(\n",
        "    n_samples=180,         # few patients\n",
        "    n_features=5000,       # many genes\n",
        "    n_informative=80,      # some truly informative genes\n",
        "    n_redundant=40,\n",
        "    n_classes=3,           # cancer subtypes\n",
        "    n_clusters_per_class=2,\n",
        "    class_sep=2.0,\n",
        "    flip_y=0.02,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# --- 1) Train / test split (stratified) ---\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "    X, y, test_size=0.25, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# --- 2) Unsupervised heuristic: how many PCs for ~95% variance? ---\n",
        "scaler_tmp = StandardScaler()\n",
        "X_tr_scaled_tmp = scaler_tmp.fit_transform(X_tr)\n",
        "pca_tmp = PCA().fit(X_tr_scaled_tmp)\n",
        "cumvar = np.cumsum(pca_tmp.explained_variance_ratio_)\n",
        "n_95 = int(np.searchsorted(cumvar, 0.95) + 1)\n",
        "\n",
        "# --- 3) Build a leakage-safe pipeline: Standardize -> PCA -> KNN ---\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(random_state=42)),\n",
        "    (\"knn\", KNeighborsClassifier())\n",
        "])\n",
        "\n",
        "# --- 4) Hyperparameter search (select n_components + KNN together) ---\n",
        "param_grid = {\n",
        "    \"pca__n_components\": [10, 20, 40, 60, 80, 100, n_95],\n",
        "    \"knn__n_neighbors\": [3, 5, 7, 9, 11],\n",
        "    \"knn__metric\": [\"euclidean\", \"manhattan\"],\n",
        "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
        "}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid = GridSearchCV(\n",
        "    pipe, param_grid=param_grid, scoring=\"accuracy\",\n",
        "    cv=cv, n_jobs=-1, refit=True, verbose=0\n",
        ")\n",
        "grid.fit(X_tr, y_tr)\n",
        "\n",
        "best_pca_knn = grid.best_estimator_\n",
        "\n",
        "# --- 5) Evaluate on held-out test set ---\n",
        "y_pred = best_pca_knn.predict(X_te)\n",
        "acc = accuracy_score(y_te, y_pred)\n",
        "macro_f1 = f1_score(y_te, y_pred, average=\"macro\")\n",
        "cm = confusion_matrix(y_te, y_pred)\n",
        "report = classification_report(y_te, y_pred, digits=4)\n",
        "\n",
        "# --- 6) Baseline: KNN without PCA (same CV protocol) ---\n",
        "pipe_no_pca = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier())\n",
        "])\n",
        "param_grid_no_pca = {\n",
        "    \"knn__n_neighbors\": [3, 5, 7, 9, 11],\n",
        "    \"knn__metric\": [\"euclidean\", \"manhattan\"],\n",
        "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
        "}\n",
        "grid_no_pca = GridSearchCV(\n",
        "    pipe_no_pca, param_grid=param_grid_no_pca, scoring=\"accuracy\",\n",
        "    cv=cv, n_jobs=-1, refit=True, verbose=0\n",
        ")\n",
        "grid_no_pca.fit(X_tr, y_tr)\n",
        "y_pred_no_pca = grid_no_pca.best_estimator_.predict(X_te)\n",
        "acc_no_pca = accuracy_score(y_te, y_pred_no_pca)\n",
        "macro_f1_no_pca = f1_score(y_te, y_pred_no_pca, average=\"macro\")\n",
        "\n",
        "# --- 7) Print concise results ---\n",
        "print(\"=== Heuristic (unsupervised) ===\")\n",
        "print(f\"PCs for ~95% variance: {n_95}\\n\")\n",
        "\n",
        "print(\"=== PCA+KNN (CV-selected) ===\")\n",
        "print(f\"Best params: {grid.best_params_}\")\n",
        "print(f\"CV mean accuracy: {grid.best_score_:.4f}\")\n",
        "print(f\"Test accuracy:    {acc:.4f}\")\n",
        "print(f\"Test macro-F1:    {macro_f1:.4f}\")\n",
        "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
        "print(cm)\n",
        "print(\"\\nClassification report:\")\n",
        "print(report)\n",
        "\n",
        "print(\"=== Baseline: KNN without PCA ===\")\n",
        "print(f\"Best params: {grid_no_pca.best_params_}\")\n",
        "print(f\"CV mean accuracy: {grid_no_pca.best_score_:.4f}\")\n",
        "print(f\"Test accuracy:    {acc_no_pca:.4f}\")\n",
        "print(f\"Test macro-F1:    {macro_f1_no_pca:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ReKZljQbDyAa",
        "outputId": "88500836-3b6f-4ce5-ccb6-82c47faf4b51"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "100 fits failed out of a total of 700.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "100 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 654, in fit\n",
            "    Xt = self._fit(X, y, routed_params, raw_params=params)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 588, in _fit\n",
            "    X, fitted_transformer = fit_transform_one_cached(\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/joblib/memory.py\", line 326, in __call__\n",
            "    return self.func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 1551, in _fit_transform_one\n",
            "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
            "    data_to_wrap = f(self, X, *args, **kwargs)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py\", line 468, in fit_transform\n",
            "    U, S, _, X, x_is_centered, xp = self._fit(X)\n",
            "                                    ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py\", line 542, in _fit\n",
            "    return self._fit_full(X, n_components, xp, is_array_api_compliant)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_pca.py\", line 556, in _fit_full\n",
            "    raise ValueError(\n",
            "ValueError: n_components=125 must be between 0 and min(n_samples, n_features)=108 with svd_solver='full'\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.3037037  0.38518519 0.36296296 0.34074074 0.37777778 0.37037037\n",
            "        nan 0.28148148 0.36296296 0.33333333 0.34074074 0.34814815\n",
            " 0.36296296        nan 0.31111111 0.34074074 0.31111111 0.34074074\n",
            " 0.42222222 0.38518519        nan 0.2962963  0.34074074 0.31111111\n",
            " 0.33333333 0.4        0.39259259        nan 0.34074074 0.34074074\n",
            " 0.33333333 0.36296296 0.38518519 0.37037037        nan 0.3037037\n",
            " 0.33333333 0.31851852 0.36296296 0.37037037 0.4               nan\n",
            " 0.3037037  0.31111111 0.27407407 0.33333333 0.33333333 0.36296296\n",
            "        nan 0.31111111 0.31111111 0.27407407 0.32592593 0.3037037\n",
            " 0.37037037        nan 0.31851852 0.31851852 0.28888889 0.36296296\n",
            " 0.39259259 0.38518519        nan 0.32592593 0.32592593 0.32592593\n",
            " 0.37037037 0.40740741 0.37037037        nan 0.31111111 0.37777778\n",
            " 0.35555556 0.37037037 0.33333333 0.28888889        nan 0.3037037\n",
            " 0.37037037 0.34814815 0.33333333 0.33333333 0.28148148        nan\n",
            " 0.27407407 0.34814815 0.35555556 0.32592593 0.37777778 0.35555556\n",
            "        nan 0.28148148 0.35555556 0.34814815 0.33333333 0.40740741\n",
            " 0.34074074        nan 0.2962963  0.33333333 0.34074074 0.28148148\n",
            " 0.34814815 0.37037037        nan 0.3037037  0.35555556 0.34074074\n",
            " 0.2962963  0.36296296 0.34074074        nan 0.2962963  0.31851852\n",
            " 0.37037037 0.31851852 0.34074074 0.37777778        nan 0.2962963\n",
            " 0.3037037  0.37037037 0.32592593 0.38518519 0.37037037        nan\n",
            " 0.2962963  0.3037037  0.35555556 0.31851852 0.37037037 0.44444444\n",
            "        nan 0.3037037  0.3037037  0.36296296 0.31111111 0.36296296\n",
            " 0.41481481        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Heuristic (unsupervised) ===\n",
            "PCs for ~95% variance: 125\n",
            "\n",
            "=== PCA+KNN (CV-selected) ===\n",
            "Best params: {'knn__metric': 'manhattan', 'knn__n_neighbors': 11, 'knn__weights': 'uniform', 'pca__n_components': 100}\n",
            "CV mean accuracy: 0.4444\n",
            "Test accuracy:    0.3333\n",
            "Test macro-F1:    0.1667\n",
            "Confusion matrix (rows=true, cols=pred):\n",
            "[[15  0  0]\n",
            " [15  0  0]\n",
            " [15  0  0]]\n",
            "\n",
            "Classification report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    1.0000    0.5000        15\n",
            "           1     0.0000    0.0000    0.0000        15\n",
            "           2     0.0000    0.0000    0.0000        15\n",
            "\n",
            "    accuracy                         0.3333        45\n",
            "   macro avg     0.1111    0.3333    0.1667        45\n",
            "weighted avg     0.1111    0.3333    0.1667        45\n",
            "\n",
            "=== Baseline: KNN without PCA ===\n",
            "Best params: {'knn__metric': 'manhattan', 'knn__n_neighbors': 3, 'knn__weights': 'distance'}\n",
            "CV mean accuracy: 0.4667\n",
            "Test accuracy:    0.3778\n",
            "Test macro-F1:    0.3652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqNYIzl3D_ao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}