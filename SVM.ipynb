{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrt-pgmFZj5p"
      },
      "outputs": [],
      "source": [
        "Ques􀆟on 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "Answer:-\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm primarily used for\n",
        "classifica􀆟on tasks, though it can also be used for regression.\n",
        "􂡈􂡉􂡊􂡋􂡌􂡍􂡎􂡏 What is SVM?\n",
        "SVM is a powerful classifica􀆟on technique that aims to find the op􀆟mal decision boundary\n",
        "(hyperplane) that separates classes in the dataset with the maximum margin.\n",
        "􂎏􂎐􂎑􂎒 How Does SVM Work?\n",
        "1. Hyperplane:\n",
        "o A hyperplane is a decision boundary that separates different classes in the\n",
        "feature space.\n",
        "o In 2D, it's a line. In 3D, it's a plane. In higher dimensions, it's a hyperplane.\n",
        "2. Maximum Margin:\n",
        "o SVM tries to find the hyperplane that not only separates the classes but also\n",
        "maximizes the distance (margin) between the nearest points (called support\n",
        "vectors) from each class and the hyperplane.\n",
        "o These support vectors are cri􀆟cal points that lie closest to the decision boundary.\n",
        "3. Support Vectors:\n",
        "o These are the data points that are closest to the hyperplane and influence its\n",
        "posi􀆟on and orienta􀆟on.\n",
        "o Removing them could change the boundary.\n",
        "4. Linear vs. Non-linear SVM:\n",
        "o If the data is linearly separable, a straight hyperplane works.\n",
        "o For non-linear data, SVM uses the kernel trick to project data into a higherdimensional\n",
        "space where a linear separa􀆟on is possible.\n",
        "􀠿􀡀 Kernels in SVM:\n",
        "SVM can use kernel func􀆟ons to handle non-linear rela􀆟onships:\n",
        " Linear kernel – for linearly separable data.\n",
        " Polynomial kernel\n",
        " Radial Basis Func􀆟on (RBF) or Gaussian kernel – most common for non-linear problems.\n",
        " Sigmoid kernel\n",
        "􀿨􀿩􀿪 Advantages:\n",
        " Effec􀆟ve in high-dimensional spaces.\n",
        " Works well when there’s a clear margin of separa􀆟on.\n",
        " Memory efficient (only uses support vectors).\n",
        "􁤶􁤷 Disadvantages:\n",
        " Not well suited for very large datasets.\n",
        " Less effec􀆟ve when data is highly noisy or overlapping.\n",
        " Requires careful tuning of hyperparameters and kernel choice.\n",
        "􀝚􀝛􀝜􀝝 Example Use Cases:\n",
        " Image classifica􀆟on\n",
        " Text categoriza􀆟on (e.g., spam detec􀆟on)\n",
        " Handwri􀆟ng recogni􀆟on\n",
        " Bioinforma􀆟cs (e.g., protein classifica􀆟on)\n",
        "Ques􀆟on 2: Explain the difference between Hard Margin and So􀅌 Margin SVM?\n",
        "Answer:-\n",
        "Difference Between Hard Margin and So􀅌 Margin SVM\n",
        "Support Vector Machines can be categorized based on how strictly they separate the classes:\n",
        "􀝲􀝳􀝴􀝵􀝶 1. Hard Margin SVM\n",
        "Defini􀆟on:\n",
        "Hard Margin SVM aims to find a hyperplane that perfectly separates the data without any\n",
        "misclassifica􀆟ons.\n",
        "􀠱􀠲􀠳􀠴 Assump􀆟ons:\n",
        " Data is linearly separable.\n",
        " No overlap between classes.\n",
        " No noise or outliers.\n",
        "􀝚􀝛􀝜􀝝 Characteris􀆟cs:\n",
        " Maximizes the margin with zero tolerance for misclassifica􀆟on.\n",
        " Very sensi􀆟ve to outliers — even one wrongly labeled point can make perfect\n",
        "separa􀆟on impossible.\n",
        "􀿨􀿩􀿪 Advantage:\n",
        " Simple and works well on clean, linearly separable data.\n",
        "􁤶􁤷 Disadvantage:\n",
        " Not usable when data is noisy or not perfectly separable.\n",
        "􀝲􀝳􀝴􀝵􀝶 2. So􀅌 Margin SVM\n",
        "Defini􀆟on:\n",
        "So􀅌 Margin SVM allows some misclassifica􀆟ons in order to find a balance between maximizing\n",
        "the margin and minimizing classifica􀆟on errors.\n",
        "􀠶􀠷􀠸􀠹 Assump􀆟ons:\n",
        " Data may not be perfectly separable.\n",
        " Allows slack (tolerance) for some points to be on the wrong side of the margin.\n",
        "􀝚􀝛􀝜􀝝 Characteris􀆟cs:\n",
        " Introduces a penalty parameter (C) that controls the trade-off between:\n",
        "o A wider margin (more tolerance)\n",
        "o Fewer misclassifica􀆟ons (less tolerance)\n",
        "􀿨􀿩􀿪 Advantage:\n",
        " More robust to noise and outliers.\n",
        " Works well on real-world data where perfect separa􀆟on is unlikely.\n",
        "􁤶􁤷 Disadvantage:\n",
        " Needs tuning of the C parameter.\n",
        " Might overfit or underfit if C is not chosen carefully.\n",
        "􁁧􁁨􁁩 Key Differences Summary Table:\n",
        "Feature Hard Margin SVM So􀅌 Margin SVM\n",
        "Misclassifica􀆟ons Not allowed Allowed (with penalty)\n",
        "Data type Perfectly linearly separable Noisy or overlapping classes\n",
        "Outlier sensi􀆟vity High Lower\n",
        "Flexibility Rigid Flexible (adjustable via C)\n",
        "􀝚􀝛􀝜􀝝 Real-World Tip:\n",
        "In prac􀆟ce, so􀅌 margin SVM is almost always used, since real-world data is rarely perfectly\n",
        "separable.\n",
        "Ques􀆟on 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use\n",
        "case?\n",
        "Answer:-\n",
        "What is the Kernel Trick in SVM?\n",
        "The Kernel Trick is a mathema􀆟cal technique used in Support Vector Machines (SVM) to allow\n",
        "the model to classify non-linearly separable data by implicitly mapping it into a higherdimensional\n",
        "space, without explicitly compu􀆟ng the transforma􀆟on.\n",
        "􀝲􀝳􀝴􀝵􀝶 Why is the Kernel Trick Useful?\n",
        " Some datasets cannot be separated using a straight line (or hyperplane) in their original\n",
        "space.\n",
        " Mapping the data to a higher dimension can make it linearly separable.\n",
        " Compu􀆟ng this transforma􀆟on directly can be expensive or infeasible.\n",
        " The kernel trick lets us compute the dot product in the higher-dimensional space\n",
        "without ever compu􀆟ng the coordinates in that space.\n",
        "This allows SVM to:\n",
        "􀿨􀿩􀿪 Handle complex pa􀆩erns\n",
        "􀿨􀿩􀿪 Work efficiently in high-dimensional feature spaces\n",
        "􀝚􀝛􀝜􀝝 How Does It Work?\n",
        "Let:\n",
        " ϕ(x)\\phi(x)ϕ(x) be the mapping func􀆟on that transforms xxx into higher dimensions.\n",
        " K(x,x􀴻)=ϕ(x)⋅ϕ(x􀴻)K(x, x') = \\phi(x) \\cdot \\phi(x')K(x,x􀴻)=ϕ(x)⋅ϕ(x􀴻) is the kernel func􀆟on,\n",
        "which computes the inner product of the transformed data without explicitly\n",
        "compu􀆟ng ϕ(x)\\phi(x)ϕ(x).\n",
        "􀢔􀢕􀢖􀢗􀢘 Example Kernel: Radial Basis Func􀆟on (RBF) / Gaussian Kernel\n",
        "􀝚􀝛􀝜􀝝 Formula:\n",
        "K(x,x􀴻)=exp⁡(−γ∥x−x􀴻∥2)K(x, x') = \\exp\\le􀅌(-\\gamma \\|x - x'\\|^2\\right)K(x,x􀴻)=exp(−γ∥x−x􀴻∥2)\n",
        " γ\\gammaγ controls the width of the kernel (how far the influence of a single training\n",
        "example reaches).\n",
        "􂡈􂡉􂡊􂡋􂡌􂡍􂡎􂡏 Use Case:\n",
        " Image classifica􀆟on, text classifica􀆟on, bioinforma􀆟cs, etc.\n",
        " Useful when the data is not linearly separable, and decision boundaries are curved or\n",
        "irregular.\n",
        "􀿨􀿩􀿪 Characteris􀆟cs:\n",
        " Maps data into infinite-dimensional space.\n",
        " Good for capturing local similari􀆟es.\n",
        " Handles complex, non-linear pa􀆩erns.\n",
        "􂎏􂎐􂎑􂎒 Summary Table:\n",
        "Kernel Type Formula Use Case\n",
        "Linear K(x,x􀴻)=x⋅x􀴻K(x, x') = x \\cdot x'K(x,x􀴻)=x⋅x􀴻 When data is linearly separable\n",
        "Polynomial\n",
        "K(x,x􀴻)=(x⋅x􀴻+c)dK(x, x') = (x \\cdot x' +\n",
        "c)^dK(x,x􀴻)=(x⋅x􀴻+c)d\n",
        "When interac􀆟ons between\n",
        "features are important\n",
        "RBF\n",
        "(Gaussian)\n",
        "exp⁡(−γ∥x−x􀴻∥2)\\exp(-\\gamma \\|x -\n",
        "x'\\|^2)exp(−γ∥x−x􀴻∥2)\n",
        "Non-linear data with complex\n",
        "pa􀆩erns\n",
        "Ques􀆟on 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "Answer:-\n",
        "What is a Naïve Bayes Classifier?\n",
        "The Naïve Bayes Classifier is a probabilis􀆟c machine learning algorithm based on Bayes'\n",
        "Theorem, used primarily for classifica􀆟on tasks.\n",
        "It calculates the probability of each class given the input features and selects the class with the\n",
        "highest probability.\n",
        "􀠆􀠇􀠈 Bayes’ Theorem Recap:\n",
        "P(C∣X)=P(X∣C)⋅P(C)P(X)P(C \\mid X) = \\frac{P(X \\mid C) \\cdot P(C)}{P(X)}P(C∣X)=P(X)P(X∣C)⋅P(C)\n",
        "Where:\n",
        " P(C∣X)P(C \\mid X)P(C∣X): Posterior probability of class CCC given input XXX\n",
        " P(X∣C)P(X \\mid C)P(X∣C): Likelihood of input XXX given class CCC\n",
        " P(C)P(C)P(C): Prior probability of class CCC\n",
        " P(X)P(X)P(X): Marginal probability of input XXX\n",
        "􂎏􂎐􂎑􂎒 Why Is It Called “Naïve”?\n",
        "It is called “naïve” because it assumes all features are condi􀆟onally independent given the\n",
        "class label — an assump􀆟on that is rarely true in real-world data.\n",
        "􀝚􀝛􀝜􀝝 In other words:\n",
        "The presence (or absence) of one feature is assumed to be unrelated to the presence (or\n",
        "absence) of any other feature, given the class.\n",
        "Despite this oversimplifica􀆟on, the algorithm o􀅌en performs surprisingly well, especially in\n",
        "high-dimensional datasets.\n",
        "􂦛􂦜􂦝􂦞􂦟􂦠􂦡􂦢􂦣􂦤􂦥􂦦 How Does It Work (in brief)?\n",
        "1. Calculate prior probabili􀆟es for each class: P(Ci)P(C_i)P(Ci)\n",
        "2. Compute likelihoods P(Xj∣Ci)P(X_j \\mid C_i)P(Xj∣Ci) for each feature XjX_jXj\n",
        "3. Apply Bayes' theorem to compute posterior probabili􀆟es P(Ci∣X)P(C_i \\mid X)P(Ci∣X)\n",
        "4. Choose the class with the highest posterior probability\n",
        "􂦂􂦃􂦄􂦅 Common Variants:\n",
        " Gaussian Naïve Bayes: Assumes features follow a normal distribu􀆟on (for con􀆟nuous\n",
        "data)\n",
        " Mul􀆟nomial Naïve Bayes: For discrete counts (e.g., word frequencies in text)\n",
        " Bernoulli Naïve Bayes: For binary features (e.g., word present/absent)\n",
        "􀿨􀿩􀿪 Advantages:\n",
        " Fast and simple\n",
        " Works well with high-dimensional data\n",
        " Requires a small amount of training data\n",
        " Performs well in text classifica􀆟on tasks (e.g., spam detec􀆟on, sen􀆟ment analysis)\n",
        "􁤶􁤷 Disadvantages:\n",
        " Strong independence assump􀆟on may hurt performance in some datasets\n",
        " Doesn’t handle feature correla􀆟on well\n",
        "􀝚􀝛􀝜􀝝 Example Use Cases:\n",
        " Email spam filtering\n",
        " Sen􀆟ment analysis\n",
        " Document classifica􀆟on\n",
        " Medical diagnosis\n",
        "Ques􀆟on 5: Describe the Gaussian, Mul􀆟nomial, and Bernoulli Naïve Bayes variants. When\n",
        "would you use each one?\n",
        "Answer:-\n",
        "Variants of Naïve Bayes Classifier:\n",
        "Naïve Bayes has three main variants, each suited for different types of data:\n",
        "􁲱􁲲􁲳 Gaussian Naïve Bayes\n",
        "􀝚􀝛􀝜􀝝 Used For:\n",
        " Con􀆟nuous (numerical) features\n",
        "􀝲􀝳􀝴􀝵􀝶 Assump􀆟on:\n",
        " Features are normally distributed (i.e., follow a Gaussian distribu􀆟on) within each class.\n",
        "􂦂􂦃􂦄􂦅 Formula:\n",
        "For a feature xxx, likelihood is calculated using the Gaussian probability density func􀆟on:\n",
        "P(x∣C)=12πσC2exp⁡(−(x−μC)22σC2)P(x \\mid C) = \\frac{1}{\\sqrt{2\\pi\\sigma_C^2}} \\exp\\le􀅌( -\n",
        "\\frac{(x - \\mu_C)^2}{2\\sigma_C^2} \\right)P(x∣C)=2πσC21exp(−2σC2(x−μC)2)\n",
        "Where:\n",
        " μC\\mu_CμC, σC2\\sigma_C^2σC2 are the mean and variance of the feature for class CCC\n",
        "􀿨􀿩􀿪 Use Case Examples:\n",
        " Iris flower classifica􀆟on\n",
        " Medical data with con􀆟nuous measurements (e.g., blood pressure, age, etc.)\n",
        "􁳁􁳂􁳃 Mul􀆟nomial Naïve Bayes\n",
        "􀝚􀝛􀝜􀝝 Used For:\n",
        " Discrete count data (especially frequency-based)\n",
        "􀝲􀝳􀝴􀝵􀝶 Assump􀆟on:\n",
        " Features represent counts or frequencies of events, such as word counts in documents.\n",
        "􂦂􂦃􂦄􂦅 Formula:\n",
        "Likelihood is based on the mul􀆟nomial distribu􀆟on, modeling how o􀅌en events occur.\n",
        "􀿨􀿩􀿪 Use Case Examples:\n",
        " Text classifica􀆟on (e.g., spam detec􀆟on, topic classifica􀆟on)\n",
        " Document classifica􀆟on based on word counts\n",
        " Bag-of-words or TF-IDF representa􀆟ons of text\n",
        "􁲽􁲾􁲿 Bernoulli Naïve Bayes\n",
        "􀝚􀝛􀝜􀝝 Used For:\n",
        " Binary/boolean features\n",
        "􀝲􀝳􀝴􀝵􀝶 Assump􀆟on:\n",
        " Features are binary values (e.g., 1 if a word occurs in a document, 0 otherwise)\n",
        "􂦂􂦃􂦄􂦅 Formula:\n",
        "Models whether a feature occurs or not, not how many 􀆟mes.\n",
        "􀿨􀿩􀿪 Use Case Examples:\n",
        " Text classifica􀆟on with binary term presence/absence\n",
        " Spam filtering based on presence/absence of keywords\n",
        " Simple yes/no feature inputs\n",
        "􁁧􁁨􁁩 Summary Table:\n",
        "Variant Feature Type Assumes... Common Use Cases\n",
        "Gaussian Con􀆟nuous\n",
        "Features follow a normal\n",
        "distribu􀆟on\n",
        "Medical data, numeric features\n",
        "Mul􀆟nomial\n",
        "Discrete\n",
        "Counts\n",
        "Feature values are\n",
        "frequencies/counts\n",
        "Text classifica􀆟on with word\n",
        "counts\n",
        "Bernoulli Binary (0/1)\n",
        "Features are binary\n",
        "(present/absent)\n",
        "Text classifica􀆟on (word\n",
        "presence)\n",
        "Dataset Info:\n",
        "● You can use any suitable datasets like Iris, Breast Cancer, or Wine from sklearn.datasets or a\n",
        "CSV file you have.\n",
        "Ques􀆟on 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "Answer:-\n",
        "Here's a complete Python program that:\n",
        " Loads the Iris dataset from sklearn.datasets\n",
        " Trains an SVM Classifier with a linear kernel\n",
        " Prints the accuracy and the support vectors\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selec􀆟on import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "# 1. Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "# 2. Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# 3. Train an SVM Classifier with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "# 4. Predict on the test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "# 5. Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "# 6. Print support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_model.support_vectors_)\n",
        "# Op􀆟onal: number of support vectors per class\n",
        "print(\"\\nNumber of Support Vectors for Each Class:\")\n",
        "print(svm_model.n_support_)\n",
        "Output\n",
        "Model Accuracy: 1.00\n",
        "Support Vectors:\n",
        "[[5.1 3.8 1.6 0.2]\n",
        "[4.6 3.2 1.4 0.2]\n",
        "...\n",
        "[5.8 2.6 4.0 1.2]]\n",
        "Number of Support Vectors for Each Class:\n",
        "[4 4 5]\n",
        "Ques􀆟on 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classifica􀆟on report including precision, recall, and F1-score.\n",
        "Answer:-\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selec􀆟on import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classifica􀆟on_report\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# 2. Split into training and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# 3. Train a Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "# 4. Predict on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "# 5. Print the classifica􀆟on report\n",
        "print(\"Classifica􀆟on Report:\\n\")\n",
        "print(classifica􀆟on_report(y_test, y_pred, target_names=data.target_names))\n",
        "Sample output\n",
        "Classifica􀆟on Report:\n",
        "precision recall f1-score support\n",
        "malignant 0.94 0.90 0.92 43\n",
        "benign 0.94 0.96 0.95 71\n",
        "accuracy 0.94 114\n",
        "macro avg 0.94 0.93 0.93 114\n",
        "weighted avg 0.94 0.94 0.94 114\n",
        "Notes:\n",
        " GaussianNB is ideal here because the features are con􀆟nuous (real-valued).\n",
        " The classifica􀆟on report includes:\n",
        "o Precision: How many predicted posi􀆟ves were correct\n",
        "o Recall: How many actual posi􀆟ves were captured\n",
        "o F1-score: Harmonic mean of precision and recall\n",
        "Ques􀆟on 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and\n",
        "gamma.\n",
        "● Print the best hyperparameters and accuracy.\n",
        "Answer:-\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selec􀆟on import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "# 1. Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# 2. Split into training and tes􀆟ng sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# 3. Define the SVM model and hyperparameter grid\n",
        "svm = SVC()\n",
        "param_grid = {\n",
        "'C': [0.1, 1, 10, 100],\n",
        "'gamma': [0.001, 0.01, 0.1, 1],\n",
        "'kernel': ['rbf']\n",
        "}\n",
        "# 4. Perform Grid Search with cross-valida􀆟on\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "# 5. Predict on the test set\n",
        "y_pred = grid_search.predict(X_test)\n",
        "# 6. Print the best hyperparameters and accuracy\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy: {:.2f}\".format(accuracy_score(y_test, y_pred)))\n",
        "􁟨􁟩􁟪􁟫􁟬􁟭􁟮􁟯􁟰 Sample Output:\n",
        "Best Hyperparameters: {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
        "Test Accuracy: 1.00\n",
        "􀠿􀡀 Notes:\n",
        " C: Controls the trade-off between smooth decision boundary and classifying training\n",
        "points correctly.\n",
        " gamma: Defines how far the influence of a single training example reaches.\n",
        " rbf (Radial Basis Func􀆟on) is good for non-linear classifica􀆟on problems.\n",
        "Ques􀆟on 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthe􀆟c text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predic􀆟ons.\n",
        "Answer:-\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extrac􀆟on.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import Mul􀆟nomialNB\n",
        "from sklearn.model_selec􀆟on import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "# 1. Load a subset of the 20 Newsgroups dataset (binary classifica􀆟on for simplicity)\n",
        "categories = ['sci.space', 'rec.sport.baseball']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "newsgroups.data, newsgroups.target, test_size=0.2, random_state=42\n",
        ")\n",
        "# 3. Convert text to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "# 4. Train a Mul􀆟nomial Naïve Bayes model\n",
        "model = Mul􀆟nomialNB()\n",
        "model.fit(X_train_vec, y_train)\n",
        "# 5. Predict probabili􀆟es for ROC-AUC\n",
        "y_proba = model.predict_proba(X_test_vec)[:, 1]\n",
        "# 6. Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "􁟨􁟩􁟪􁟫􁟬􁟭􁟮􁟯􁟰 Sample Output:\n",
        "ROC-AUC Score: 0.9842\n",
        "􀿨􀿩􀿪 Notes:\n",
        " We use only two categories to simplify the ROC-AUC computa􀆟on (binary classifica􀆟on).\n",
        " predict_proba returns class probabili􀆟es — needed for ROC-AUC.\n",
        " You can expand this to mul􀆟-class ROC-AUC using label_binarize() if needed.\n",
        "Ques􀆟on 10: Imagine you’re working as a data scien􀆟st for a company that handles email\n",
        "communica􀆟ons.\n",
        "Your task is to automa􀆟cally classify emails as Spam or Not Spam.\n",
        "The emails may contain:\n",
        "● Text with diverse vocabulary\n",
        "● Poten􀆟al class imbalance (far more legi􀆟mate emails than spam)\n",
        "● Some incomplete or missing data Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectoriza􀆟on, handling missing data)\n",
        "● Choose and jus􀆟fy an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solu􀆟on with suitable metrics And explain the business\n",
        "impact of your solu􀆟on.\n",
        "Answer:-\n",
        "Certainly! Here's a detailed approach to building a Spam vs. Not Spam email classifier,\n",
        "including preprocessing, model choice, handling class imbalance, evalua􀆟on, and business\n",
        "value.\n",
        "􀿨􀿩􀿪 1. Data Preprocessing\n",
        "􀶠􀶡 a. Handling Missing Data\n",
        " Missing email text: Drop those entries if text is essen􀆟al.\n",
        " Missing metadata (subject, sender): Use placeholder values or impute if features are\n",
        "important.\n",
        " Par􀆟al emails: Use available parts (e.g., just body or subject) as input.\n",
        "􀶠􀶡 b. Text Cleaning\n",
        " Lowercasing, removing punctua􀆟on, HTML tags, and stopwords\n",
        " Op􀆟onal: Apply stemming or lemma􀆟za􀆟on to reduce word forms\n",
        "􀶠􀶡 c. Vectoriza􀆟on\n",
        "Use TF-IDF Vectoriza􀆟on to convert text into numeric features:\n",
        "from sklearn.feature_extrac􀆟on.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
        "􀿨􀿩􀿪 2. Model Choice and Jus􀆟fica􀆟on\n",
        "􁇤􁇥􁇦􁇧􁇨 Naïve Bayes (Mul􀆟nomialNB)\n",
        " Pros:\n",
        "o Fast and efficient for large text corpora\n",
        "o Works well with sparse and high-dimensional data (like word counts)\n",
        "o Surprisingly robust despite \"naïve\" assump􀆟ons\n",
        " Cons:\n",
        "o Assumes independence between features\n",
        "􁇤􁇥􁇦􁇧􁇨 SVM with Linear Kernel\n",
        " Pros:\n",
        "o Works well in high-dimensional spaces\n",
        "o Effec􀆟ve in separa􀆟ng classes with a clear margin\n",
        " Cons:\n",
        "o Slower with large datasets\n",
        "o Harder to interpret, especially probabili􀆟es (needs probability=True)\n",
        "􀿨􀿩􀿪 Recommenda􀆟on:\n",
        " Start with Naïve Bayes for baseline performance and simplicity\n",
        " Use SVM if Naïve Bayes underperforms or requires more margin-based learning\n",
        "􀿨􀿩􀿪 3. Handling Class Imbalance\n",
        " Use class weigh􀆟ng in SVM:\n",
        "SVC(class_weight='balanced')\n",
        " Or use resampling techniques:\n",
        "o Oversample spam using SMOTE or similar\n",
        "o Undersample the non-spam class\n",
        " Alterna􀆟vely, try ensemble models (e.g., Random Forest with class weights)\n",
        "􀿨􀿩􀿪 4. Model Evalua􀆟on\n",
        "Use metrics suitable for imbalanced binary classifica􀆟on:\n",
        "Metric Why it's useful\n",
        "Precision How many predicted spams are actually spam\n",
        "Recall How many actual spams were detected\n",
        "F1-score Balance between precision and recall\n",
        "ROC-AUC Measures overall discrimina􀆟on performance\n",
        "Confusion Matrix Insight into false posi􀆟ves/nega􀆟ves\n",
        "python\n",
        "CopyEdit\n",
        "from sklearn.metrics import classifica􀆟on_report, roc_auc_score, confusion_matrix\n",
        "􀿨􀿩􀿪 5. Business Impact of the Solu􀆟on\n",
        "Impact Area Descrip􀆟on\n",
        "􁇤􁇥􁇦􁇧􁇨 Produc􀆟vity Reduces manual sor􀆟ng, saving employee 􀆟me and a􀆩en􀆟on\n",
        "􀠣􀠤􀠥􀠦􀠧􀠨􀠩􀠪 Security Flags poten􀆟al phishing/spam threats, reducing cyber risk\n",
        "􀝓􀝔􀝕􀝖􀝗􀝘 Customer Trust Prevents cri􀆟cal emails from being missed due to spam clu􀆩er\n",
        "􀛜􀛝􀛞􀛟 Cost Reduc􀆟on Minimizes human effort and email server overload caused by spam filtering\n",
        "􂞙􂞚􂞛􂞜 Data Insights Helps analyze spam pa􀆩erns for future improvements\n",
        "􁐙􁐚􁐛 Summary\n",
        "Step Tool / Technique\n",
        "Preprocessing Text cleaning, TF-IDF, impute/drop missing\n",
        "Model Choice Naïve Bayes (fast baseline), SVM (robust)\n",
        "Class Imbalance Class weights, SMOTE, or undersampling\n",
        "Evalua􀆟on Metrics Precision, Recall, F1, ROC-AUC, Confusion Matrix\n",
        "Business Value Improved efficiency, security, and insights"
      ]
    }
  ]
}