# -*- coding: utf-8 -*-
"""Assigenment DataToolkit.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M7aAcMJQGhQVrQbVYS7qy2EZNJZbcr-n

Q1 What is NumPy, and why is it widely used in Python?
**Ans:-**
NumPy (Numerical Python) is a powerful open-source library for numerical computing in Python. It provides a high-performance multidimensional array object, as well as a wide range of functions for mathematical operations, linear algebra, random number generation, and much more.

Key Features of NumPy
Multidimensional Arrays (ndarray)

At the core of NumPy is the ndarray, a fast and flexible n-dimensional array.

Supports arrays of any dimension (1D, 2D, 3D, etc.) with efficient memory usage and optimized performance.

Example:

import numpy as np
arr = np.array([[1, 2], [3, 4]])
print(arr)
Mathematical and Statistical Operations

NumPy provides a wide array of mathematical and statistical functions like mean, median, standard deviation, trigonometric functions, etc.

Example:

data = np.array([1, 2, 3, 4, 5])
print(np.mean(data))  # Average
print(np.std(data))   # Standard deviation
Broadcasting

Allows operations on arrays of different shapes without the need for explicit looping.

Example:

a = np.array([1, 2, 3])
b = 2
print(a * b)  # [2 4 6]
Efficient Memory Usage and Performance

NumPy is written in C and Fortran, making it significantly faster than pure Python for numerical computations.

Linear Algebra and Matrix Operations

Includes powerful tools for matrix operations, eigenvalue computation, and singular value decomposition (SVD).

Example:

matrix = np.array([[1, 2], [3, 4]])
print(np.linalg.inv(matrix))  # Inverse of the matrix
Interoperability with Other Libraries

NumPy forms the foundation for many other scientific computing libraries like SciPy, pandas, and scikit-learn.

Q2 How does broadcasting work in NumPy?

**Ans:-**
 Broadcasting in NumPy is a powerful feature that allows operations on arrays of different shapes without the need for explicit looping or complex reshaping. It automatically expands the smaller array to match the shape of the larger array, making code more concise and efficient.

Key Rules for Broadcasting
Two arrays are compatible for broadcasting if:

Trailing Dimensions Match: Starting from the last dimension and moving towards the first, the dimensions of the arrays should either be equal or one of them should be 1.

Singleton Dimensions are "Stretched": If a dimension is 1 in one array, it is virtually stretched to match the size of the corresponding dimension in the other array.

Examples of Broadcasting
1. Simple Scalar Broadcasting

import numpy as np

a = np.array([1, 2, 3])
b = 2
print(a + b)
Output:


[3 4 5]
Here, the scalar 2 is "broadcast" to match the shape of a, resulting in element-wise addition.

2. One-Dimensional Broadcasting

a = np.array([1, 2, 3])
b = np.array([4])
print(a + b)
Output:


[5 6 7]
The shape of b is (1,), which is broadcast to (3,) to match a.

3. Two-Dimensional Broadcasting

a = np.array([[1, 2, 3], [4, 5, 6]])
b = np.array([10, 20, 30])
print(a + b)
Output:


[[11 22 33]
 [14 25 36]]
The shape of a is (2, 3) and b is (3,). NumPy automatically broadcasts b to (2, 3) for element-wise addition.

4. Mismatched Dimensions

a = np.array([[1, 2, 3], [4, 5, 6]])
b = np.array([[10], [20]])
print(a + b)
Output:

[[11 12 13]
 [24 25 26]]
The shape of a is (2, 3) and b is (2, 1). NumPy stretches the second dimension of b to match the columns of a.

When Broadcasting Fails
If the arrays have incompatible shapes that can't be stretched to match, NumPy raises a ValueError. For example:

a = np.array([1, 2, 3])
b = np.array([[1, 2], [3, 4]])
print(a + b)
Output:

ValueError: operands could not be broadcast together with shapes (3,) (2,2)

Q3 What is a Pandas DataFrame?

**Ans:-**
A Pandas DataFrame is a two-dimensional, size-mutable, tabular data structure in Python, widely used for data manipulation and analysis. It is a core data structure in the Pandas library, which is built on top of NumPy.

Key Features of a Pandas DataFrame
Tabular Data Structure

Organized in a table format with labeled rows and columns, similar to a spreadsheet or SQL table.

Each column can hold different data types (e.g., integers, floats, strings).

Supports both integer and labeled indexing.

Flexible Indexing

Allows both numerical (position-based) and label-based (index) access to data.

Efficient Data Manipulation

Built-in methods for filtering, grouping, merging, reshaping, and aggregating data.

Integration with Other Libraries

Easily integrates with NumPy, Matplotlib, Seaborn, and Scikit-learn for seamless data analysis.

Q4  Explain the use of the groupby() method in Pandas?

Ans:-
The groupby() method in Pandas is a powerful tool for grouping data and performing operations on those groups. It is similar to the GROUP BY clause in SQL, allowing you to split a DataFrame into smaller groups based on the values in one or more columns, apply a function to each group, and then combine the results into a new DataFrame or Series.

Key Steps in the Groupby Process
Splitting: Divide the DataFrame into groups based on some criteria.

Applying: Apply a function (e.g., sum, mean, count) to each group.

Combining: Combine the results into a single DataFrame or Series.
"""

#1. Basic Grouping

import pandas as pd

data = {
    "Department": ["Sales", "Sales", "HR", "HR", "IT", "IT"],
    "Employee": ["Alice", "Bob", "Charlie", "David", "Eve", "Frank"],
    "Salary": [50000, 60000, 55000, 58000, 75000, 72000]
}

df = pd.DataFrame(data)

# Group by "Department" and calculate the average salary
avg_salary = df.groupby("Department")["Salary"].mean()
print(avg_salary)

#2. Grouping by Multiple Columns
data = {
    "Department": ["Sales", "Sales", "HR", "HR", "IT", "IT"],
    "Location": ["NY", "SF", "NY", "SF", "NY", "SF"],
    "Salary": [50000, 60000, 55000, 58000, 75000, 72000]
}

df = pd.DataFrame(data)

# Group by both Department and Location, and calculate the average salary
avg_salary_multi = df.groupby(["Department", "Location"])["Salary"].mean()
print(avg_salary_multi)

#3. Using Aggregate Functions
# Calculate multiple statistics at once
agg_stats = df.groupby("Department")["Salary"].agg(["mean", "sum", "count"])
print(agg_stats)

#4. Iterating Over Groups
grouped = df.groupby("Department")
for name, group in grouped:
    print(f"Department: {name}")
    print(group)
    print()

"""Commonly Used Aggregation Functions
sum() - Total of each group

mean() - Average of each group

count() - Number of non-null values in each group

max() - Maximum value in each group

min() - Minimum value in each group

median() - Median value in each group

std() - Standard deviation of each group

When to Use groupby()
Data Aggregation: Summarizing data by categories.

Data Analysis: Calculating statistics for different groups.

Data Transformation: Applying custom functions to groups.

Exploratory Data Analysis (EDA): Understanding the distribution of data.

Q5 Why is Seaborn preferred for statistical visualizations?

Ans:-
Seaborn is a powerful Python library built on top of Matplotlib that is widely preferred for statistical visualizations due to its high-level, aesthetically pleasing, and easy-to-use interface. It is specifically designed to work well with Pandas DataFrames, making it ideal for exploratory data analysis (EDA) and statistical data visualization.

Key Reasons Why Seaborn is Preferred
High-Level Abstractions for Statistical Graphics

Simplifies the creation of complex statistical plots with just a few lines of code.

Functions like sns.histplot(), sns.boxplot(), sns.violinplot(), and sns.pairplot() are tailored for statistical insights.

Example:
import seaborn as sns
import matplotlib.pyplot as plt

# Sample Data
tips = sns.load_dataset("tips")

# Basic distribution plot
sns.histplot(data=tips, x="total_bill", kde=True)
plt.show()
Beautiful and Consistent Styling

Seaborn provides attractive default styles and color palettes, which reduce the need for extensive customization.

Built-in themes like darkgrid, whitegrid, and ticks make plots look professional.

Example:

sns.set_theme(style="whitegrid")
sns.boxplot(data=tips, x="day", y="total_bill")
plt.show()
Advanced Statistical Analysis

Seaborn can visualize complex statistical relationships, including regression analysis with sns.regplot() and categorical plots like sns.catplot().

Example (regression plot):

sns.lmplot(data=tips, x="total_bill", y="tip", hue="sex")
plt.show()
Automatic Handling of DataFrames

Directly works with Pandas DataFrames, automatically using column names as labels.

No need for manual data extraction, making the code cleaner and more intuitive.

Powerful Multi-Plot Grids

Functions like sns.pairplot() and sns.FacetGrid() allow for efficient exploration of pairwise relationships and complex multi-plot visualizations.

Example (pair plot):

sns.pairplot(data=tips, hue="sex")
plt.show()
Customization and Flexibility

Although simple to use, Seaborn is highly customizable, allowing for fine-grained control over plots if needed.

Easily integrates with Matplotlib for more advanced tweaks.

Q6 What are the differences between NumPy arrays and Python lists?

Ans:-
Both NumPy arrays and Python lists are used to store collections of items, but they have significant differences in terms of functionality, performance, and use cases. Here’s a detailed comparison:

Key Differences Between NumPy Arrays and Python Lists
Feature	NumPy Array	Python List
Data Type	Fixed, homogeneous (all elements must be of the same type)	Dynamic, heterogeneous (elements can be of different types)
Speed	Much faster (C-optimized)	Slower (pure Python)
Memory Efficiency	More efficient (compact memory layout)	Less efficient (pointers and type information)
Mathematical Operations	Element-wise operations, matrix math	No built-in support for element-wise math
Dimensionality	Supports multi-dimensional arrays (1D, 2D, 3D, etc.)	Typically 1D (can nest lists, but less efficient)
Vectorization	Yes, supports broadcasting and vectorized operations	No, requires loops or list comprehensions
Built-in Functions	Rich mathematical functions (sin, cos, mean, etc.)	Limited, mainly basic methods like append() and extend()
Memory Allocation	Contiguous memory blocks (efficient cache usage)	Scattered memory blocks (less cache friendly)
Indexing	Supports advanced, multidimensional, and boolean indexing	Basic, one-dimensional indexing

Speed Comparison
Python List:

python
Copy
Edit
import time

size = 1_000_000
py_list = list(range(size))

start = time.time()
py_list = [x * 2 for x in py_list]
print("Python List Time:", time.time() - start)
NumPy Array:

python
Copy
Edit
import numpy as np

np_array = np.arange(size)

start = time.time()
np_array = np_array * 2
print("NumPy Array Time:", time.time() - start)
Typical Output:

sql
Copy
Edit
Python List Time: 0.050 seconds  
NumPy Array Time: 0.002 seconds
NumPy is significantly faster because it uses C-optimized code and vectorized operations.

Memory Efficiency
Python List:

python
Copy
Edit
import sys
py_list = [1] * 1000
print("Memory used by Python list:", sys.getsizeof(py_list), "bytes")
NumPy Array:

python
Copy
Edit
np_array = np.ones(1000, dtype=int)
print("Memory used by NumPy array:", np_array.nbytes, "bytes")
Output:

yaml
Copy
Edit
Memory used by Python list: 8056 bytes  
Memory used by NumPy array: 8000 bytes
NumPy arrays are more memory-efficient because they store elements in contiguous blocks of memory.

Mathematical Operations
Python List:

python
Copy
Edit
a = [1, 2, 3]
b = [4, 5, 6]
c = [x + y for x, y in zip(a, b)]
print(c)
NumPy Array:

python
Copy
Edit
a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
c = a + b
print(c)
Output:

csharp
Copy
Edit
[5, 7, 9]
NumPy allows element-wise addition directly, while Python lists require explicit loops or comprehensions.

Multi-Dimensional Support
Python List:

python
Copy
Edit
matrix = [[1, 2], [3, 4], [5, 6]]
print(matrix[1][1])  # 4
NumPy Array:

python
Copy
Edit
matrix = np.array([[1, 2], [3, 4], [5, 6]])
print(matrix[1, 1])  # 4
NumPy arrays provide a more natural way to work with multi-dimensional data.

When to Use Which?
Use NumPy Array	Use Python List
Numerical data	Heterogeneous data
Large datasets	Small to moderate datasets
High performance required	Flexibility needed
Scientific computing	General-purpose programming

Q7 What is a heatmap, and when should it be used?

**Ans:- ** A heatmap is a data visualization technique that uses color-coding to represent the magnitude of a variable across two dimensions. It's often used to visualize user behavior on websites, showing where users click, scroll, or hover, or to represent patterns in data like population density or temperatures.

What it is:
Heatmaps use a color gradient to show how a variable changes across different areas or categories. For example, a website heatmap might use warmer colors to indicate areas where users click more frequently, while cooler colors represent areas with less interaction.

When to use it:
Heatmaps are valuable for:
Analyzing user behavior on websites: Identifying areas of high and low engagement, optimizing layouts, and improving user experience according to Optimizely.

Representing geographical data: Visualizing population density, temperature variations, or other spatial patterns.
Analyzing correlations between variables: Identifying relationships between different factors, such as stock prices or product performance.

Improving business processes: Analyzing foot traffic in retail stores, optimizing production flow, or identifying areas for improvement.
Simplifying complex data: Transforming intricate datasets into easily understandable visuals.

Key features:
Color gradients: Usually, warmer colors (like red, yellow) represent higher values, while cooler colors (like blue, green) represent lower values.
Two-dimensional representation: Heatmaps typically display data across two axes, such as a grid of cells or a map with geographical coordinates.
Intuitive visualization: They make it easy to quickly identify patterns, trends, and areas of interest within the data.

Q8 What does the term “vectorized operation” mean in NumPy?

**Ans:-**
 Vectorized operations in NumPy enable the use of efficient, pre-compiled functions and mathematical operations on NumPy arrays and data sequences. Vectorization is a method of performing array operations without the use of for loops.

 Vectorized operations in NumPy refer to performing mathematical and logical operations on entire arrays (or vectors) without the need for explicit loops. This approach is highly optimized and takes full advantage of low-level C and Fortran implementations, resulting in much faster execution compared to standard Python loops.

Key Features of Vectorized Operations
Speed: Vectorized operations are significantly faster than loops due to efficient, low-level implementation.

Code Simplicity: Cleaner and more concise code, closer to mathematical notation.

Memory Efficiency: Operates directly on NumPy arrays without creating temporary Python objects.

Broadcasting: Automatically handles operations on arrays of different shapes without explicit looping.

Q9 How does Matplotlib differ from Plotly?

**Ans:-**
Matplotlib and Plotly are two of the most popular data visualization libraries in Python, but they have distinct differences in terms of features, design, and use cases.

Key Differences Between Matplotlib and Plotly

| Feature            | **Matplotlib**                                      | **Plotly**                                                                    |
| ------------------ | --------------------------------------------------- | ----------------------------------------------------------------------------- |
| **Interactivity**  | Static, with basic interactivity via `mpl_toolkits` | Highly interactive, built for modern web browsers                             |
| **Output Format**  | PNG, PDF, SVG                                       | HTML, PNG, PDF, SVG, JSON                                                     |
| **Ease of Use**    | High-level API but more manual customization needed | High-level, intuitive API with rich built-in interactivity                    |
| **3D Plotting**    | Basic 3D support (`mpl_toolkits.mplot3d`)           | Advanced, fully interactive 3D plots                                          |
| **Customization**  | Highly customizable                                 | Highly customizable, with integrated themes                                   |
| **Integration**    | Strong integration with scientific libraries        | Seamless integration with web frameworks (Dash, Flask)                        |
| **Speed**          | Faster for small to medium datasets                 | Generally slower for very large datasets due to the overhead of interactivity |
| **Learning Curve** | Moderate to steep                                   | Moderate, but more intuitive for interactive charts                           |

Q10 What is the significance of hierarchical indexing in Pandas?

**Ans:-** Hierarchical Indexing (also known as MultiIndex) is a powerful feature in Pandas that allows you to create multi-level indexes for rows and columns, providing a way to work with higher-dimensional data in a compact, structured manner. This feature is particularly useful for complex data analysis, where you need to represent and manipulate data with multiple categorical levels.

Key Benefits of Hierarchical Indexing in Pandas
Organization of Complex Data

Allows you to structure data along multiple dimensions.

Ideal for representing data with multiple levels of grouping, like time-series data with multiple hierarchies (e.g., year, month, day).

Efficient Data Slicing

Enables intuitive, concise data filtering and selection using .loc[] and .xs().

Improved Data Aggregation

Simplifies group-wise operations, making it easier to compute aggregates at different levels.

Flexibility with Pivot Tables

Facilitates the creation of complex pivot tables and reshaping of data.

Clearer Data Representation

Makes data tables more readable and meaningful by providing context through multiple levels.
"""

#Creating a MultiIndex DataFrame
import pandas as pd
import numpy as np

arrays = [["USA", "USA", "Canada", "Canada"], ["East", "West", "East", "West"]]
index = pd.MultiIndex.from_arrays(arrays, names=["Country", "Region"])

data = [100, 150, 80, 120]
df = pd.DataFrame({"Sales": data}, index=index)
print(df)

"""Q11 What is the role of Seaborn’s pairplot() function?

Ans:-The pairplot() function in Seaborn is a powerful tool for exploratory data analysis (EDA). It is used to visualize pairwise relationships in a dataset, providing a quick and comprehensive view of the interactions between multiple variables.

Key Features of pairplot()
Pairwise Scatter Plots

Plots scatter plots for each pair of numerical variables.

Helps identify correlations and patterns.

Diagonal Distribution Plots

Shows univariate distributions (e.g., histograms or KDEs) on the diagonal.

Color Grouping

Allows grouping by a categorical variable using the hue parameter.

Highly Customizable

Supports customization of plot styles, colors, and marker sizes.

Efficient Data Exploration

Provides a compact way to visualize complex datasets with just one line of code.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Load built-in dataset
iris = sns.load_dataset("iris")

# Simple pairplot
sns.pairplot(iris)
plt.show()

"""Q12What is the purpose of the describe() function in Pandas?

**Ans:-**
The describe() function in Pandas is used to generate summary statistics of a DataFrame or Series. It provides a quick overview of the distribution and key characteristics of numerical (and optionally categorical) data, which is extremely useful during exploratory data analysis (EDA).

What Does describe() Provide?
By default, for numerical columns, it returns:

count: Number of non-missing values

mean: Average value

std: Standard deviation (measure of spread)

min: Minimum value

25%: 25th percentile (first quartile)

50%: Median (second quartile)

75%: 75th percentile (third quartile)

max: Maximum value

For categorical or object columns, if you use describe(include='object') or include='all', it returns:

count: Number of non-missing values

unique: Number of unique categories

top: Most frequent category

freq: Frequency of the most frequent category
"""

import pandas as pd

df = pd.DataFrame({
    'Age': [23, 45, 31, 35, 22, 40],
    'Salary': [50000, 80000, 60000, 65000, 48000, 70000]
})

print(df.describe())

"""Q13Why is handling missing data important in Pandas?

**Ans:-**
Handling missing data in Pandas is crucial because missing or incomplete data can significantly impact the accuracy, reliability, and validity of your data analysis and modeling. Here's why it's important:

Why Handling Missing Data Matters
Prevents Errors in Computation

Many functions and algorithms may fail or return incorrect results if missing values (NaN) are present.

For example, mathematical operations might propagate NaNs and skew summaries or predictions.

Improves Data Quality and Integrity

Missing data can bias your analysis if not properly addressed, leading to misleading conclusions.

Cleaning or imputing missing values ensures your dataset reflects the true underlying patterns.

Enables Accurate Statistical Analysis

Statistical measures like mean, median, correlations, and regressions require complete data or proper handling of missing values to be valid.

Ignoring missing data or handling it incorrectly can distort these measures.

Enhances Model Performance

Many machine learning models cannot handle missing values directly and require preprocessing.

Proper handling (e.g., imputation, removal) improves model training and predictions.

Maintains Consistency

Ensures consistent dataset size and structure, which is critical for reproducibility and comparability across analyses.

Common Strategies in Pandas for Handling Missing Data
"""

#Detecting Missing Values:
df.isnull()      # Returns True where data is missing
df.notnull()     # Returns True where data is present

#Removing Missing Data:
df.dropna()      # Drops rows with missing values
df.dropna(axis=1) # Drops columns with missing values

#Filling Missing Data (Imputation):
df.fillna(0)              # Replace NaNs with 0
df.fillna(method='ffill') # Forward fill
df.fillna(method='bfill') # Backward fill

"""Summary
Handling missing data properly is essential to ensure:

Accurate, reliable analysis and results

Cleaner, more usable datasets

Robust and effective machine learning models

Q14 What are the benefits of using Plotly for data visualization?

**Ans:-**
Plotly offers several benefits that make it a popular choice for data visualization, especially when you want interactive and web-ready graphics. Here are the key advantages:

Benefits of Using Plotly for Data Visualization
Interactive Visualizations

Plots allow zooming, panning, hovering, and clicking to explore data dynamically.

Tooltips show detailed info on hover without cluttering the plot.

Wide Variety of Plot Types

Supports basic plots (scatter, line, bar) and advanced ones like 3D plots, heatmaps, choropleth maps, and more.

Specialized charts for finance, scientific data, and geospatial analysis.

Web and Dashboard Integration

Easily integrates with web frameworks like Dash, Flask, and Django for building interactive dashboards and apps.

Outputs HTML files that can be embedded in websites or Jupyter notebooks.

High-Quality, Publication-Ready Graphics

Produces visually appealing and polished charts with professional aesthetics.

Supports extensive customization of layouts, colors, annotations, and themes.

Cross-Platform Compatibility

Works seamlessly in Jupyter notebooks, standalone Python scripts, and web applications.

Also available for R, MATLAB, and JavaScript.

Easy Sharing and Exporting

Export charts as static images (PNG, SVG, PDF) or interactive HTML files.

Facilitates collaboration by sharing interactive charts easily.

Open Source with Strong Community

Active development and community support.

Plenty of examples, tutorials, and documentation.

Summary
Plotly is ideal if you need interactive, shareable, and visually rich data visualizations, especially for web or dashboard applications.

Q15  How does NumPy handle multidimensional arrays?
**Ans:-**
NumPy handles multidimensional arrays by providing a powerful ndarray object, which is a fixed-size, homogeneous (all elements of the same type) container for data arranged in N dimensions. This allows efficient storage, manipulation, and computation on data structures beyond simple 1D arrays (vectors), such as matrices (2D), tensors (3D and higher), and more.

How NumPy Handles Multidimensional Arrays
ndarray Structure

A NumPy array can have any number of dimensions, specified by its shape (a tuple of integers).

For example, a 2D array has shape (rows, columns), a 3D array might have (depth, rows, columns), etc.

Shape and Dimensions

The shape attribute tells you the size along each dimension.

The ndim attribute gives the number of dimensions (axes).

Memory Layout

NumPy arrays are stored in contiguous blocks of memory (row-major order by default, like C arrays).

This allows very fast indexing, slicing, and operations.

Indexing and Slicing

Supports multi-dimensional indexing: arr[2, 3] accesses the element in row 2, column 3 of a 2D array.

Slicing can be done across any dimension, e.g., arr[:, 1:4] slices all rows but only columns 1 to 3.

Broadcasting and Vectorized Operations

Supports operations between arrays of different shapes by automatically expanding smaller arrays (broadcasting).

Enables fast element-wise arithmetic and functions without explicit loops.

Creation and Manipulation

You can create arrays of any dimension using functions like np.array(), np.zeros(), np.ones(), np.random.rand(), specifying the shape.

Reshaping, flattening, transposing, and swapping axes are all supported with methods like .reshape(), .flatten(), .transpose(), .swapaxes().
"""

import numpy as np

# Create a 3D array with shape (2, 3, 4)
arr = np.arange(24).reshape(2, 3, 4)

print("Array shape:", arr.shape)
print("Number of dimensions:", arr.ndim)
print("Array contents:\n", arr)

# Access element at depth=1, row=2, column=3
print("Element at (1, 2, 3):", arr[1, 2, 3])

# Slice: all depths, second row, all columns
print("Slice arr[:, 1, :]:\n", arr[:, 1, :])

"""Q16 What is the role of Bokeh in data visualization?

**Ans:-**
Bokeh is a powerful Python library designed for creating interactive, browser-based visualizations. Its primary role is to help users build rich, dynamic plots and dashboards that can be easily embedded in web applications or shared as standalone HTML files.

Key Roles of Bokeh in Data Visualization:
Interactive Visualizations

Provides tools like zooming, panning, hovering, and selecting data points interactively.

Users can explore data dynamically, making it great for presentations and dashboards.

Web-Ready Outputs

Generates visualizations as HTML and JavaScript that run in modern web browsers without requiring users to install anything.

Plots can be embedded in websites, Jupyter notebooks, or standalone files.

Server-Backed Applications

Supports building interactive web apps with Python backend logic through Bokeh Server.

Enables real-time updates and complex user interactions.

Rich Plot Types and Widgets

Supports a wide variety of plots: line, scatter, bar, heatmaps, geographic maps, network graphs, etc.

Includes UI widgets like sliders, dropdowns, and buttons to control visualizations dynamically.

Integration with Other Tools

Works well with Pandas, NumPy, and other scientific Python libraries.

Can be combined with Flask, Django, or other web frameworks for custom apps.

Customizable and Extensible

Offers fine control over plot appearance and interactivity.

Allows custom JavaScript callbacks for advanced interactions.

Summary
Bokeh’s role is to provide interactive, flexible, and web-friendly visualizations that empower users to create data-rich applications and dashboards with minimal effort.

Q17 Both apply() and map() in Pandas are used to apply functions to data, but they serve different purposes and work on different types of Pandas objects.

map()
Works only on Pandas Series (i.e., a single column).

Used primarily to map values of a Series according to an input correspondence, such as a dictionary, a Series, or a function.

Often used for element-wise transformations or value substitution.

Returns a Series of the same shape as the original.
"""

import pandas as pd

s = pd.Series(['cat', 'dog', 'rabbit'])

# Map values using a dictionary
mapping = {'cat': 'feline', 'dog': 'canine'}
print(s.map(mapping))
# Output: feline, canine, NaN (because 'rabbit' is not in mapping)

"""###apply()
Works on both Series and DataFrames.

When used on a Series, it applies a function element-wise (like map() but more flexible).

When used on a DataFrame, it can apply a function along either axis (axis=0 for columns, axis=1 for rows).

Can be used for complex row-wise or column-wise operations.###
"""

s = pd.Series([1, 2, 3])
print(s.apply(lambda x: x**2))  # squares each element

df = pd.DataFrame({
    'A': [1, 2],
    'B': [3, 4]
})

# Sum across columns for each row
print(df.apply(sum, axis=1))

"""| Feature       | `map()`                              | `apply()`                                           |
| ------------- | ------------------------------------ | --------------------------------------------------- |
| Works on      | Series only                          | Series and DataFrame                                |
| Functionality | Element-wise mapping or substitution | Apply function element-wise or along rows/columns   |
| Input         | Dict, Series, or function            | Any callable function                               |
| Use case      | Value replacement, simple mapping    | Complex transformations, row/column-wise operations |

Q18 What are some advanced features of NumPy?

**Ans:-** Here are some advanced features of NumPy that make it a powerful tool for scientific computing and data analysis beyond basic array operations:

Advanced Features of NumPy
Broadcasting

Enables arithmetic operations between arrays of different shapes without explicit loops.

Greatly simplifies code and improves performance.

Structured Arrays and Record Arrays

Allows storage of heterogeneous data types (like columns with different types) in a single array.

Useful for working with tabular or database-like data efficiently.

Memory Mapping (memmap)

Handles large datasets that don’t fit into memory by mapping files on disk as arrays.

Enables reading/writing parts of huge arrays without loading everything into RAM.

Universal Functions (ufuncs)

Fast, vectorized functions operating element-wise on arrays (e.g., np.sin, np.exp).

Supports broadcasting and type casting automatically.

Advanced Indexing and Fancy Indexing

Allows selecting data using integer arrays, boolean masks, or conditional statements.

Enables powerful, concise data filtering and manipulation.

Linear Algebra Support

Provides functions for matrix multiplication, inversion, eigenvalues, singular value decomposition (SVD), etc., via numpy.linalg.

Random Number Generation

Offers flexible, reproducible random number generation through the numpy.random module, including distributions beyond uniform and normal.

FFT (Fast Fourier Transform)

Computes discrete Fourier transforms efficiently using numpy.fft.

Polynomials Module

Tools for working with polynomial functions, including fitting and evaluation.

Integration with C/C++ and Fortran

Provides ways to interface with lower-level languages for speed-critical code using tools like ctypes and Cython.

Masked Arrays

Supports arrays with missing or invalid entries masked out during computations.

Shape Manipulation and Views

Powerful tools like reshape(), ravel(), transpose(), and the concept of views vs copies allow flexible data manipulation without extra memory cost.

Vectorized String Operations

numpy.char module offers vectorized string operations on arrays of strings.

Q19 How does Pandas simplify time series analysis?

**Ans:-**
Pandas simplifies time series analysis by providing powerful, easy-to-use tools tailored specifically for handling and analyzing time-indexed data. Here’s how it makes working with time series much easier:

How Pandas Simplifies Time Series Analysis
Datetime Indexing

Pandas can use DatetimeIndex, PeriodIndex, or TimedeltaIndex to label data points with timestamps, periods, or time durations.

This enables fast and intuitive slicing, filtering, and alignment based on dates and times.

Flexible Date Parsing

Functions like pd.to_datetime() convert strings or other formats into datetime objects easily, handling many formats and even fuzzy parsing.

Resampling and Frequency Conversion

resample() lets you change the frequency of time series data (e.g., from daily to monthly), applying aggregation functions like sum, mean, or custom ones.

Supports upsampling (inserting new time points) and downsampling (reducing data frequency).

Time Zone Handling

Supports time zone-aware datetime objects and easy conversion between time zones with tz_localize() and tz_convert().

Rolling and Expanding Windows

Provides rolling() and expanding() methods to compute moving averages, sums, or other window-based statistics, which are essential for smoothing and trend analysis.

Date Range Generation

date_range() and bdate_range() generate sequences of dates or business days, useful for creating time indexes or missing date placeholders.

Period Arithmetic and Alignment

Enables arithmetic operations on dates and periods, including offsetting, shifting (shift()), and aligning multiple time series easily.

Handling Missing Data in Time Series

Functions to interpolate, forward-fill (ffill), or back-fill (bfill) missing timestamps or values.


Pandas provides:

Intuitive date/time indexing and slicing

Flexible frequency conversion and resampling

Robust missing data handling and interpolation

Built-in rolling window statistics

Easy time zone and period management
"""

#Resampling Daily Data to Monthly Mean

import pandas as pd

# Create a time series with daily data
dates = pd.date_range('2023-01-01', periods=10, freq='D')
data = pd.Series(range(10), index=dates)

# Resample to monthly frequency and calculate mean
monthly_data = data.resample('ME').mean()

print(monthly_data)

"""Q20 What is the role of a pivot table in Pandas?

**Ans:-**
A pivot table in Pandas is a powerful tool used to summarize, aggregate, and reorganize data in a DataFrame, making it easier to analyze relationships between variables.

Role of a Pivot Table in Pandas
Aggregates Data: Groups data by one or more keys (rows and/or columns) and applies aggregation functions (like sum, mean, count) on other columns.

Reshapes Data: Converts data from a long format into a more compact, table-like format with hierarchical rows and columns.

Facilitates Multi-dimensional Analysis: Allows easy exploration of data by summarizing across multiple dimensions simultaneously.

Simplifies Complex Grouping: Provides a shortcut to perform group-by and pivot operations in one step.

Improves Data Visualization Prep: Prepares data in a summarized form suitable for charts and dashboards.

Basic Syntax
pd.pivot_table(data, values=None, index=None, columns=None, aggfunc='mean', fill_value=None)
data: The DataFrame to pivot

values: Column(s) to aggregate

index: Column(s) to make new rows

columns: Column(s) to make new columns

aggfunc: Aggregation function(s), default is 'mean'

fill_value: Value to replace missing entries
"""

import pandas as pd

data = pd.DataFrame({
    'Region': ['East', 'East', 'West', 'West'],
    'Product': ['A', 'B', 'A', 'B'],
    'Sales': [100, 150, 200, 250],
    'Quantity': [10, 15, 20, 25]
})

pivot = pd.pivot_table(data, values='Sales', index='Region', columns='Product', aggfunc='sum')
print(pivot)

"""Q21 Why is NumPy’s array slicing faster than Python’s list slicing?

**Ans:-**
NumPy’s array slicing is faster than Python’s list slicing primarily because of the way NumPy arrays are stored and accessed in memory. Here’s why:

Key Reasons for NumPy’s Speed Advantage
Contiguous Memory Layout (Locality of Reference)

NumPy arrays are stored in contiguous blocks of memory, unlike Python lists, which are arrays of pointers to objects.

This results in better CPU cache efficiency, reducing memory access time during slicing.

Faster access to elements since no extra pointer dereferencing is needed.

Homogeneous Data Types

NumPy arrays have a single, fixed data type, allowing optimized, low-level machine code execution.

No need for dynamic type checking during slicing, unlike Python lists which can hold mixed types.

Vectorized Operations

NumPy is built on highly optimized C and Fortran libraries that leverage SIMD (Single Instruction, Multiple Data) operations.

Slicing operations are implemented in efficient, compiled code, avoiding the overhead of Python loops.

Views Instead of Copies

NumPy slices are views of the original data, not copies, meaning they share the same underlying memory.

This reduces memory usage and speeds up slicing significantly.

In contrast, slicing a Python list creates a new list, requiring more time and memory.
"""

import numpy as np
import time

# Creating large NumPy array and Python list
arr = np.arange(10**6)
lst = list(range(10**6))

# Timing NumPy slicing
start = time.time()
_ = arr[100000:200000]
print("NumPy slicing time:", time.time() - start)

# Timing Python list slicing
start = time.time()
_ = lst[100000:200000]
print("List slicing time:", time.time() - start)

"""Q22 What are some common use cases for Seaborn?

**Ans:-**
Seaborn is a powerful Python visualization library built on top of Matplotlib, designed for statistical data visualization. It provides a high-level interface for drawing attractive and informative graphics, making it particularly useful for exploratory data analysis (EDA) and statistical insights.

Common Use Cases for Seaborn
Exploratory Data Analysis (EDA)

Quickly understand the distribution, relationships, and structure of your data.

Ideal for discovering patterns, trends, and outliers.

Visualizing Distributions

Plot histograms, KDE plots, and ECDFs to explore the shape and spread of data.

Use sns.histplot(), sns.kdeplot(), sns.displot().

Categorical Data Analysis

Analyze relationships between categorical and numerical variables using bar plots, box plots, and violin plots.

Use sns.barplot(), sns.boxplot(), sns.violinplot().

Pairwise Relationships

Visualize pairwise relationships in a dataset using pair plots or joint plots.

Use sns.pairplot() or sns.jointplot().

Correlation and Heatmaps

Create heatmaps to visualize correlation matrices or other grid-like data.

Use sns.heatmap() for clear, color-coded summaries.

Regression Analysis

Plot linear relationships with confidence intervals using sns.regplot() or sns.lmplot().

Useful for identifying trends and testing hypotheses.

Time Series and Trends

Plot time series data with line plots and area plots.

Use sns.lineplot() for straightforward time series analysis.

Facet Grids and Small Multiples

Create complex, multi-dimensional visualizations using sns.FacetGrid() or sns.catplot() to display multiple subplots.

Data Exploration with Complex Datasets

Use joint grids, pair grids, and clustermaps to visualize higher-dimensional data relationships.

Customization and Theming

Easily customize plots with built-in themes, palettes, and styles (sns.set_theme()).
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Load built-in Iris dataset
iris = sns.load_dataset("iris")

# Pairplot to explore relationships
sns.pairplot(iris, hue="species", diag_kind="kde")
plt.show()

"""# **Practical**"""

#How do you create a 2D NumPy array and calculate the sum of each row.
import numpy as np

# Creating a 2D array from a nested list
arr = np.array([[1, 2, 3],
                [4, 5, 6],
                [7, 8, 9]])

print("2D Array:")
print(arr)


#Calculating Row Sums
row_sums = np.sum(arr, axis=1)
print("\nRow Sums:", row_sums)


#Alternative Methods for Row Sums

row_sums = arr.sum(axis=1)

row_sums = [sum(row) for row in arr]

#Write a Pandas script to find the mean of a specific column in a DataFrame
import pandas as pd

# Sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'Salary': [50000, 60000, 70000, 80000]
}

df = pd.DataFrame(data)

# Find the mean of the 'Salary' column
mean_salary = df['Salary'].mean()

print("Mean Salary:", mean_salary)

#Create a scatter plot using Matplotlib
import matplotlib.pyplot as plt
import numpy as np

# Generate some random data
np.random.seed(42)
x = np.random.rand(50) * 100
y = np.random.rand(50) * 200

# Create scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(x, y, color='teal', marker='o', alpha=0.7, edgecolors='black')
plt.title("Basic Scatter Plot")
plt.xlabel("X Values")
plt.ylabel("Y Values")
plt.show()

#How do you calculate the correlation matrix using Seaborn and visualize it with a heatmap
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Sample DataFrame
data = {
    'Age': [25, 30, 35, 40, 45],
    'Salary': [50000, 60000, 70000, 80000, 90000],
    'Experience': [1, 5, 10, 15, 20],
    'Satisfaction': [7, 8, 6, 9, 10]
}

df = pd.DataFrame(data)

# Calculate the correlation matrix
corr_matrix = df.corr()
print(corr_matrix)

##Visualizing with a Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title("Correlation Matrix Heatmap")
plt.show()

plt.figure(figsize=(8, 6))
sns.heatmap(corr_matrix, annot=True, cmap='YlGnBu', linewidths=0.3, square=True, fmt='.2f')
plt.title("Correlation Matrix Heatmap")
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.show()

#Generate a bar plot using Plotly
import plotly.express as px
import pandas as pd

# Sample data
data = {
    'Category': ['A', 'B', 'C', 'D', 'E'],
    'Values': [50, 80, 30, 60, 90]
}

df = pd.DataFrame(data)

# Create a bar plot
fig = px.bar(df, x='Category', y='Values', title='Basic Bar Plot',
             color='Values', color_continuous_scale='viridis')

# Show the plot
fig.show()

fig = px.bar(df,
             x='Category',
             y='Values',
             title='Customized Bar Plot',
             color='Values',
             color_continuous_scale='Blues',
             text='Values')

fig.update_layout(
    xaxis_title="Product Category",
    yaxis_title="Sales Volume",
    title_font_size=20,
    template='plotly_dark'
)

fig.show()

#Create a DataFrame and add a new column based on an existing column
import pandas as pd

# Creating a DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David'],
    'Age': [25, 30, 35, 40],
    'Salary': [50000, 60000, 70000, 80000]
}

df = pd.DataFrame(data)
print("Original DataFrame:")
print(df)

# Adding a new column 'Age Group' based on the 'Age' column
df['Age Group'] = df['Age'].apply(lambda x: 'Young' if x < 30 else 'Experienced')
print("\nDataFrame with Age Group:")
print(df)

# Adding a column for annual tax (10% of Salary)
df['Tax'] = df['Salary'] * 0.1
print("\nDataFrame with Tax Column:")
print(df)

df['Income Level'] = df['Salary'].apply(lambda x: 'High' if x > 65000 else 'Moderate')
print("\nDataFrame with Income Level Column:")
print(df)

#Write a program to perform element-wise multiplication of two NumPy arrays
import numpy as np

# Creating two NumPy arrays
arr1 = np.array([1, 2, 3, 4, 5])
arr2 = np.array([10, 20, 30, 40, 50])

# Element-wise multiplication
result = arr1 * arr2

print("Array 1:", arr1)
print("Array 2:", arr2)
print("Element-wise Multiplication Result:", result)

result = np.multiply(arr1, arr2)
print("Element-wise Multiplication (using np.multiply):", result)

#Create a line plot with multiple lines using Matplotlib
import matplotlib.pyplot as plt

# Sample data
years = [2018, 2019, 2020, 2021, 2022]
sales_A = [50, 60, 70, 80, 90]
sales_B = [30, 45, 65, 85, 95]
sales_C = [20, 40, 60, 80, 100]

# Create a line plot
plt.figure(figsize=(10, 6))
plt.plot(years, sales_A, marker='o', label='Product A', color='blue', linewidth=2)
plt.plot(years, sales_B, marker='s', label='Product B', color='green', linewidth=2)
plt.plot(years, sales_C, marker='^', label='Product C', color='red', linewidth=2)

# Adding titles and labels
plt.title("Yearly Sales Comparison", fontsize=18)
plt.xlabel("Year", fontsize=14)
plt.ylabel("Sales (in thousands)", fontsize=14)

# Adding grid and legend
plt.grid(True, linestyle='--', alpha=0.6)
plt.legend(loc='upper left')

# Show the plot
plt.show()

fig, ax1 = plt.subplots(figsize=(10, 6))

ax1.plot(years, sales_A, marker='o', color='blue', label='Product A')
ax1.plot(years, sales_B, marker='s', color='green', label='Product B')
ax1.set_ylabel("Sales (in thousands)")

ax2 = ax1.twinx()
ax2.plot(years, sales_C, marker='^', color='red', label='Product C')
ax2.set_ylabel("Product C Sales")

plt.title("Yearly Sales Comparison (with Dual Y-Axis)")
fig.legend(loc='upper left')
plt.show()

#Generate a Pandas DataFrame and filter rows where a column value is greater than a threshold
import pandas as pd

# Create a sample DataFrame
data = {
    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],
    'Age': [25, 32, 18, 47, 29],
    'Salary': [50000, 62000, 48000, 75000, 54000]
}

df = pd.DataFrame(data)

# Define the threshold
salary_threshold = 55000

# Filter rows where Salary > threshold
filtered_df = df[df['Salary'] > salary_threshold]

print("Filtered DataFrame (Salary > 55000):")
print(filtered_df)

#Create a histogram using Seaborn to visualize a distribution
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Generate some sample data
data = np.random.normal(loc=0, scale=1, size=1000)

# Create the histogram
plt.figure(figsize=(8, 6))
sns.histplot(data, bins=30, kde=True, color='purple')

# Add title and labels
plt.title("Histogram with KDE")
plt.xlabel("Value")
plt.ylabel("Frequency")

plt.show()

#Perform matrix multiplication using NumPy
import numpy as np

# Define two matrices (2D arrays)
A = np.array([[1, 2, 3],
              [4, 5, 6]])

B = np.array([[7, 8],
              [9, 10],
              [11, 12]])

# Matrix multiplication using @ operator
result = A @ B

print("Matrix A:")
print(A)

print("\nMatrix B:")
print(B)

print("\nMatrix multiplication result (A @ B):")
print(result)

#Use Pandas to load a CSV file and display its first 5 rowsA
import pandas as pd

# Load the CSV file into a DataFrame
df = pd.read_csv('C:\Users\Ravindra Pal\Downloads\example.csv')

# Display the first 5 rows
print(df.head())

#Create a 3D scatter plot using Plotly
import plotly.express as px
import pandas as pd
import numpy as np

# Generate sample data
np.random.seed(42)
df = pd.DataFrame({
    'x': np.random.randn(50),
    'y': np.random.randn(50),
    'z': np.random.randn(50),
    'category': np.random.choice(['A', 'B', 'C'], size=50)
})

# Create 3D scatter plot
fig = px.scatter_3d(df, x='x', y='y', z='z',
                    color='category',
                    title='3D Scatter Plot with Plotly',
                    symbol='category',
                    size_max=10)

fig.show()

